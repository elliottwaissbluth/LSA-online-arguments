{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversations gone awry, Wikipedia version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /home/gaoag/.convokit/downloads/conversations-gone-awry-corpus\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, download\n",
    "import spacy\n",
    "import pickle\n",
    "corpus = Corpus(filename=download('conversations-gone-awry-corpus'))\n",
    "\n",
    "# # Load liwc_dic\n",
    "# with open('liwc_dic.pkl', 'rb') as handle:\n",
    "#     liwc_dic = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 8069\n",
      "Number of Utterances: 30021\n",
      "Number of Conversations: 4188\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrB.TG\n"
     ]
    }
   ],
   "source": [
    "utt = corpus.random_utterance()\n",
    "print(utt.speaker.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation('id': '626028390.17390.17390', 'utterances': ['626028390.17390.17390', '626028390.17411.17390', '626112636.17657.17657', '626113273.17905.17905', '626352468.18039.18039', '626388808.18147.18147'], 'meta': {'page_title': 'Talk:Gary Cooper', 'page_id': 19720668, 'pair_id': '637956280.2202.2202', 'conversation_has_personal_attack': True, 'verified': False, 'pair_verified': False, 'annotation_year': '2019', 'split': 'val'})\n"
     ]
    }
   ],
   "source": [
    "convo = corpus.random_conversation()\n",
    "print(convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['626028390.17390.17390', '626028390.17411.17390', '626112636.17657.17657', '626113273.17905.17905', '626352468.18039.18039', '626388808.18147.18147']\n",
      "['Doctor Papa Jones', 'Doctor Papa Jones', 'Doctor Papa Jones', 'Doctor Papa Jones', 'JimFrads', 'Doctor Papa Jones']\n"
     ]
    }
   ],
   "source": [
    "paths = convo.get_longest_paths()\n",
    "for path in paths:\n",
    "    print([utt.id for utt in path])\n",
    "    print([utt.get_speaker().id for utt in path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utterance features\n",
    "- **id**: index of the utterance\n",
    "- **conversation_id**: id of the first utterance in the converstaion this utterance belongs to\n",
    "- **reply-to**: index of the utterance to which this utterance replies to (None if not a reply)\n",
    "- **speaker**: the speaker who authored the utterance\n",
    "- **timestamp**: timestamp of utterance\n",
    "- **text**: textual content of the utterance\n",
    "- **meta**: metadata for each utterance\n",
    "    - **is_section_header**: whether the utterance is a conversation \"title\" or \"subject\" (if true, the utterance should be ignored)\n",
    "    - **comment_has_personal_attack**: whether this comment was judged by 3 crowdsourced annotators to contain a personal comment_has_personal_attack\n",
    "    - **parsed**: SpaCy parsed version of the utterance text\n",
    "        - **rt**: ??\n",
    "        - **toks**: List of parsed tokens\n",
    "            - **tok**: the token (word, punctuation, etc.)\n",
    "            - **tag**: Detailed part of speech tag\n",
    "            - **dep**: syntactic dependency, i.e. the relation between the tokens\n",
    "            - **up**: list related to dn, not sure how\n",
    "            - **dn**: list related to up, not sure how"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation features\n",
    "\n",
    "- **id**: id of the conversation\n",
    "- **utterances**: ids of utterances in the conversation (in order I presume)\n",
    "- **meta**: conversation metadata\n",
    "    - **page_title**: title of page under which conversation is occurring\n",
    "    - **page_id**: unique numerical id of the talk page\n",
    "    - **pair_id**: the id of the conversation that this comment's conversation is paired with\n",
    "    - **conversation_has_personal_attack**: whether any comment in this comment's conversation contains a personal attack\n",
    "    - **verified**: whether the personal attack label has been verified by an internal annotator\n",
    "    - **pair_verified**: whether the personal attack label has been double checked by the internal annotator\n",
    "    - **annotation_year**: self explanatory\n",
    "    - **split**: (train, test, or val) whether this conversation was used as train, test, or val in \"Trouble on the Horizon\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to find the conversations that are easy to analyze, i.e. have a structure like (a -> b -> a -> b -> ...). detect_interlocution should reveal those conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(637/4188) 15.2% conversations valid\n"
     ]
    }
   ],
   "source": [
    "# We want to consider conversations with a call-reply structure between two speakers, having at least five utterances\n",
    "def detect_interlocution(conv, min_utts, print=False):\n",
    "    '''\n",
    "    Finds whether the conversation has a call-reply structure between two speakers with at least min_utts utterances\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > conv - entire conversation object\n",
    "        > min_utts - the minimum number of utterances that constitute a valid conversation\n",
    "        > print - whether or not to print why the conversation was rejected\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > bool representing whether or not the conversation's longest path has the aforementioned structure\n",
    "    '''\n",
    "    # At the moment, only considering first longest path if there are multiple # TODO: Add functionality to examine all paths\n",
    "    try:\n",
    "        longest_path = conv.get_longest_paths()[0]\n",
    "    except ValueError as v:\n",
    "        if print:\n",
    "            print(v)\n",
    "            print('skipping...')\n",
    "        return False\n",
    "    \n",
    "    if len(longest_path) < min_utts:\n",
    "        if print:\n",
    "            print('Less than {} utterances in conversation\\nskipping...'.format(min_utts))\n",
    "        return False\n",
    "\n",
    "    speakers = []\n",
    "\n",
    "    for utt in longest_path:\n",
    "        speakers.append(utt.get_speaker().id)\n",
    "    \n",
    "    if len(set(speakers)) > 2:\n",
    "        if print:\n",
    "            print('More than 2 speakers in conversation\\nskipping...')\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Determine number of valid conversations\n",
    "num_valid = 0\n",
    "valid_conv_ids = []  # Will hold IDs of all valid converations\n",
    "for conv in corpus.iter_conversations():\n",
    "    if detect_interlocution(conv, 5):\n",
    "        valid_conv_ids.append(conv.id)\n",
    "        num_valid += 1\n",
    "\n",
    "print('({}/{}) {:.1f}% conversations valid'.format(num_valid, len(corpus.get_conversation_ids()), num_valid*100/len(corpus.get_conversation_ids())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get a single conversation from list of valid IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation({'obj_type': 'conversation', 'meta': {'page_title': 'User talk:AngryParsley', 'page_id': 1282978, 'pair_id': '12941035.584.584', 'conversation_has_personal_attack': False, 'verified': True, 'pair_verified': True, 'annotation_year': '2018', 'split': 'train'}, 'vectors': [], 'tree': <convokit.model.utteranceNode.UtteranceNode object at 0x7f0ed853f220>, 'owner': <convokit.model.corpus.Corpus object at 0x7f0f3da26940>, 'id': '12451425.436.436'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_conversation(valid_conv_ids[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of valid conversations, let's find a way to isolate the utterances in a way that we can easily analyze for style accommodation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': '66.36.136.123',\n",
       " 'b': 'Mike Garcia',\n",
       " 'a_b': [('15832773.3035.3019', '15832939.3151.3151'),\n",
       "  ('15833000.3256.3256', '15833036.3275.3275')],\n",
       " 'b_a': [('15832939.3151.3151', '15833000.3256.3256')]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_speaker_utt_lists(conv):\n",
    "    '''\n",
    "    Generates lists of speaker IDs corresponding to utterances in conv, and gets utterances\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > conv - entire conversation object\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > speakers - list of speakers corresponding to each utterance\n",
    "        > utts - list of utterances from conversation\n",
    "    '''\n",
    "    # Recall, we use the first index longest path in our list of valid conversations\n",
    "    longest_path = conv.get_longest_paths()[0]\n",
    "\n",
    "    speakers = []\n",
    "    utts = []\n",
    "\n",
    "    for utt in longest_path:\n",
    "        utts.append(utt.id)\n",
    "        speakers.append(utt.get_speaker().id)\n",
    "\n",
    "    return speakers, utts\n",
    "\n",
    "def get_pairs(speakers, utts):\n",
    "    '''\n",
    "    Generates a dictionary of pairs of utterances, each pair representing a back and forth interaction\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > speakers - list of speakers corresponding to each utterance\n",
    "        > utts - list of utterances from conversation\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > pairs - dictionary with the following structure\n",
    "            a : ID of speaker a\n",
    "            b : ID of speaker b\n",
    "            a_b : [(tuple of 2 utterance IDs, first being from speaker a and second from speaker b), (...), ...]\n",
    "            b_a : [(tuple of 2 utterance IDs, first being from speaker b and second from speaker a), (...), ...]\n",
    "    '''\n",
    "    # TODO: Account for instances where a speaker speaks multiple times in a row. Combine those into a list of utteranes within the tuples\n",
    "    pairs = {\n",
    "        'a' : list(set(speakers))[0],\n",
    "        'b' : list(set(speakers))[1],\n",
    "        'a_b' : [],\n",
    "        'b_a' : []\n",
    "    }\n",
    "\n",
    "    # We'll say speaker a is the first speaker, and speaker b is the second.False\n",
    "    speaker_shift = [1 if speakers[i] != speakers[i-1] else 0 for i in range(1, len(speakers))]\n",
    "    speaker_shift.insert(0,0) # Prepend 0 (first utterance isn't a response)\n",
    "\n",
    "    for i in range(1, len(speakers)):\n",
    "        if speakers[i] == pairs['b'] and speakers[i-1] == pairs['a']:\n",
    "            pairs['a_b'].append((utts[i-1], utts[i]))\n",
    "        elif speakers[i] == pairs['a'] and speakers[i-1] == pairs['b']:\n",
    "            pairs['b_a'].append((utts[i-1], utts[i]))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "speakers, utts = get_speaker_utt_lists(corpus.get_conversation(valid_conv_ids[3]))\n",
    "get_pairs(speakers, utts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure style accommodation we have to measure the style markers in each utterance. That's what this following function is for.\n",
    "\n",
    "see https://spacy.io/models/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conjunction, subordinating or preposition'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('IN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy and ARK tags\n",
    "\n",
    "\n",
    "| ARK tag | spaCy tag \t| our tag \t| intended definition \t| actual                                   \t|\n",
    "|-------- |-----------\t|---------\t|---------------------\t|------------------------------------------\t|\n",
    "| O       | PRP       \t| ppron   \t| personal pronoun    \t| personal pronoun                         \t|\n",
    "|         |           \t| ipron   \t| impersonal pronoun  \t|                                          \t|\n",
    "|         |           \t| article \t| article             \t|                                          \t|\n",
    "| &       | CC        \t| conj    \t| conjunction         \t| coordinating conjunction                 \t|\n",
    "| P       | IN        \t| prep    \t| preposition         \t| subordinating or preposition conjunction \t|\n",
    "| V       | MD        \t| auxverb \t| auxiliary verb      \t| modal auxiliary verb                     \t|\n",
    "| R       | RB        \t| adverb  \t| common adverb       \t| adverb                                   \t|\n",
    "|         |           \t| negate  \t| negation            \t|                                          \t|\n",
    "|         |           \t| quant   \t| quantifier          \t|                                          \t|\n",
    "|N        |             |           | noun                  |                                           |  \n",
    "|^        |             |           | proper noun           |                                           |\n",
    "|S        |             |           | nominal+possessive    |                                           |\n",
    "|Z        |             |           | proper noun+posessive |                                           |\n",
    "|L        |             |           | nominal+verbal        |                                           |\n",
    "|M        |             |           | proper noun+verbal    |                                           |\n",
    "|A        |             |           | adjective             |                                           |\n",
    "|!        |             |           | interjection          | use                                       |\n",
    "|D        |             |           | determiner            |                                           |\n",
    "|T        |             |           | verb particle         |                                           |\n",
    "|X        |             |           | existential \"there\"   |                                           |\n",
    "|Y        |             |           | x+verbal              |                                           |\n",
    "|#        |             |           | hashtag               |                                           |\n",
    "|@        |             |           | @ mention             |                                           |\n",
    "|~        |             |           | discourse marker      | use (can't figure it out though)          |\n",
    "|U        |             |           | URL or email          |                                           |\n",
    "|E        |             |           | emoticon              | use                                       |\n",
    "|$        |             |           | numeral               |                                           |\n",
    "|,        |             |           | punctuation           | use                                       |\n",
    "|G        |             |           | garbage/other         |                                           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /home/gaoag/.convokit/downloads/reddit-corpus-small\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download(\"reddit-corpus-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = corpus.get_conversation('9c0sn1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'66.36.136.123', 'Mike Garcia'}\n",
      "b'Just because Mike Garcia believes this to be true does not mean it should be in the article . I follow SOAD news very carefully and this has never been an issue , let alone being a confirmed fact as Mike Garcia seems to think it is . I would like to see a source or have this deleted .\\tR P ^ ^ V O P V A V R V O V V P D N , O V ^ ^ R R & O V R V D N , V A V D A N P ^ ^ V P V O V , O V V P V D N & V O V ,\\t0.9105 0.7939 0.9991 0.9993 0.8976 0.6920 0.7857 0.9884 0.7016 0.5884 0.6126 0.8534 0.8597 0.9777 0.9816 0.8058 0.9086 0.9073 0.9837 0.9637 0.9852 0.9692 0.4307 0.7684 0.6488 0.9707 0.7212 0.9007 0.4368 0.8187 0.4237 0.9597 0.9794 0.7247 0.3480 0.4681 0.8406 0.8676 0.9272 0.6490 0.9987 0.9983 0.6706 0.6733 0.9968 0.9290 0.9600 0.9631 0.9674 0.9917 0.9270 0.7918 0.9641 0.9338 0.9036 0.8031 0.9154 0.6213 0.6162 0.9813\\tJust because Mike Garcia believes this to be true does not mean it should be in the article. I follow SOAD news very carefully and this has never been an issue, let alone being a confirmed fact as Mike Garcia seems to think it is. I would like to see a source or have this deleted. \\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'O': 1,\n",
       " 'V': 1,\n",
       " 'R': 1,\n",
       " '!': 0,\n",
       " 'P': 1,\n",
       " '&': 1,\n",
       " 'E': 0,\n",
       " ',': 1,\n",
       " 'ipron': 0,\n",
       " 'article': 0,\n",
       " 'negate': 0,\n",
       " 'quant': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import subprocess\n",
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def get_style_markers_LIWC(utt):\n",
    "    '''\n",
    "    Returns a dictionary containing the number of style markers in an utterance\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > utt - a single utterance\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > m - dictionary with the following key value pairs\n",
    "            ppron : # personal pronouns\n",
    "            ipron : # impersonal pronouns\n",
    "            article : # articles\n",
    "            conj : # conjunctions\n",
    "            prep : # prepositions\n",
    "            auxverb : # auxiliary verbs\n",
    "            adverb : # common adverbs\n",
    "            negate : # negations\n",
    "            quant : # quantifiers\n",
    "    '''\n",
    "    m = {\n",
    "        'ppron' : 0,\n",
    "        'ipron' : 0,\n",
    "        'article' : 0,\n",
    "        'conj' : 0,\n",
    "        'prep' : 0,\n",
    "        'auxverb' : 0,\n",
    "        'adverb' : 0,\n",
    "        'negate' : 0,\n",
    "        'quant' : 0\n",
    "    }\n",
    "\n",
    "    # Tokenize text\n",
    "    text = utt.text.lower().split()\n",
    "\n",
    "    # Analyze using the LIWC keys\n",
    "    for word in text:\n",
    "        if word in liwc_dic.keys():\n",
    "            m[liwc_dic[word]] += 1\n",
    "    \n",
    "    # Convert to boolean\n",
    "    for k in m.keys():\n",
    "        if m[k]:\n",
    "            m[k] = 1\n",
    "\n",
    "    return m\n",
    "\n",
    "def get_style_markers_EXTENDED(utt):\n",
    "    '''\n",
    "    Returns a dictionary containing the number of style markers in an utterance\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > utt - a single utterance\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > m - dictionary with the following key value pairs - (outlined in the table above)\n",
    "        should probably select for ones that are somewhat indicative of style - G and @, for instance, probably don't matter\n",
    "            \n",
    "    '''\n",
    "    m = {\n",
    "#         'N' : 0,\n",
    "        'O' : 0,\n",
    "#         '^' : 0,\n",
    "#         'S' : 0,\n",
    "#         'Z' : 0,\n",
    "        'V' : 0,\n",
    "#         'L' : 0,\n",
    "#         'M' : 0,\n",
    "#         'A' : 0,\n",
    "        'R' : 0,\n",
    "        '!' : 0,\n",
    "#         'D' : 0,\n",
    "        'P' : 0,\n",
    "        '&' : 0,\n",
    "#         'T' : 0,\n",
    "#         'X' : 0,\n",
    "#         'Y' : 0,\n",
    "#         '#' : 0,\n",
    "#         '@' : 0,\n",
    "#         '~' : 0,\n",
    "#         'U' : 0,\n",
    "        'E' : 0,\n",
    "#         '$' : 0,\n",
    "        ',' : 0,\n",
    "#         'G' : 0,\n",
    "        'ipron' : 0,\n",
    "        'article' : 0,\n",
    "        'negate' : 0,\n",
    "        'quant' : 0\n",
    "    }\n",
    "    \n",
    "    text_demojied_with_marker = emoji_pattern.sub('wkkemojification', utt.text)\n",
    "    \n",
    "   \n",
    "    # save utt text to file in the appropriate format\n",
    "    temp_file_obj = open(\"tempinput.txt\", 'w')\n",
    "    temp_file_obj.write(text_demojied_with_marker)\n",
    "    temp_file_obj.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # subprocess the file to the java jar; pipe output to a script that returns a wordXPOS dict \n",
    "    proc = subprocess.Popen(['../ark-tweet-nlp/runTagger.sh', './tempinput.txt'], stdout=subprocess.PIPE)\n",
    "#     output = subprocess.check_output(['~/nlp/ark-tweet-nlp/scripts/'], stdin = runtagger_call.stdout)\n",
    "    while True:\n",
    "        line = proc.stdout.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        print(line)\n",
    "        tokens, tags, conf, orig = [l.decode(\"utf-8\") for l in line.split(b'\\t')]\n",
    "        for tok, tag in zip(tokens, tags):\n",
    "            if tag in m.keys():\n",
    "                m[tag] += 1\n",
    "#             if tok in liwc_dic.keys() and liwc_dic[tok] in m.keys():\n",
    "#                 m[liwc_dic[tok]] += 1\n",
    "                \n",
    "    # Convert to boolean\n",
    "    for k in m.keys():\n",
    "        if m[k]:\n",
    "            m[k] = 1\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "\n",
    "speakers, utts = get_speaker_utt_lists(corpus.get_conversation(valid_conv_ids[3]))\n",
    "print(set(speakers))\n",
    "get_style_markers_EXTENDED(corpus.get_utterance(utts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs:  {'a': 'The undertow', 'b': 'Indubitably', 'a_b': [('162766001.2432.2409', '162770113.2620.2620'), ('162775518.2849.2849', '162776180.2956.2956'), ('162791351.3026.3026', '162791625.3109.3109')], 'b_a': [('162770113.2620.2620', '162775518.2849.2849'), ('162776180.2956.2956', '162791351.3026.3026')]}\n",
      "\n",
      "raw_b_a:  {'ppron': 1, 'ipron': 2, 'article': 3, 'conj': 2, 'prep': 2, 'auxverb': 2, 'adverb': 3, 'negate': 1, 'quant': 0}\n",
      "raw_a_b:  {'ppron': 2, 'ipron': 2, 'article': 2, 'conj': 2, 'prep': 1, 'auxverb': 2, 'adverb': 2, 'negate': 2, 'quant': 0}\n",
      "\n",
      "elicit_b_a:  {'ppron': 0.3333333333333333, 'ipron': 0.6666666666666666, 'article': 1.0, 'conj': 0.6666666666666666, 'prep': 0.6666666666666666, 'auxverb': 0.6666666666666666, 'adverb': 1.0, 'negate': 0.3333333333333333, 'quant': 0.0}\n",
      "elicit_a_b:  {'ppron': 0.5, 'ipron': 1.0, 'article': 1.0, 'conj': 0.5, 'prep': 0.5, 'auxverb': 1.0, 'adverb': 1.0, 'negate': 0.5, 'quant': 0.0}\n",
      "\n",
      "baseline_b_a:  {'ppron': 0.3333333333333333, 'ipron': 0.6666666666666666, 'article': 1.0, 'conj': 0.6666666666666666, 'prep': 0.6666666666666666, 'auxverb': 0.6666666666666666, 'adverb': 1.0, 'negate': 0.3333333333333333, 'quant': 0.0}\n",
      "baseline_a_b:  {'ppron': 1.0, 'ipron': 1.0, 'article': 1.0, 'conj': 1.0, 'prep': 0.5, 'auxverb': 1.0, 'adverb': 1.0, 'negate': 1.0, 'quant': 0.0}\n",
      "\n",
      "C_b_a:  {'ppron': 0.0, 'ipron': 0.0, 'article': 0.0, 'conj': 0.0, 'prep': 0.0, 'auxverb': 0.0, 'adverb': 0.0, 'negate': 0.0, 'quant': None}\n",
      "C_a_b:  {'ppron': 0.5, 'ipron': 0.0, 'article': 0.0, 'conj': 0.5, 'prep': 0.0, 'auxverb': 0.0, 'adverb': 0.0, 'negate': 0.5, 'quant': None}\n",
      "\n",
      "LSM:  {'ppron': 0.6666777774074197, 'ipron': 1.0, 'article': 0.8000039999200016, 'conj': 1.0, 'prep': 0.6666777774074197, 'auxverb': 1.0, 'adverb': 0.8000039999200016, 'negate': 0.6666777774074197, 'quant': None}\n",
      "\n",
      "mean_C_b_a:  0.0\n",
      "mean_C_a_b:  0.1875\n"
     ]
    }
   ],
   "source": [
    "def initialize_dict():\n",
    "    return {\n",
    "        'ppron' : 0,\n",
    "        'ipron' : 0,\n",
    "        'article' : 0,\n",
    "        'conj' : 0,\n",
    "        'prep' : 0,\n",
    "        'auxverb' : 0,\n",
    "        'adverb' : 0,\n",
    "        'negate' : 0,\n",
    "        'quant' : 0\n",
    "    }\n",
    "\n",
    "def measure_coordination(conv, print_output=False):\n",
    "    '''\n",
    "    Assumes the converation will only have two speakers\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > conv - entire conversation object\n",
    "        > print_output - whether to print medial variables\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > C - dictionary with following key value pairs\n",
    "            convID : ID of conversation\n",
    "            a : ID of speaker a\n",
    "            b : ID of speaker b\n",
    "            num_response_b_a : number of responses from b to a\n",
    "            num_response_a_b : number of responses from a to b\n",
    "            C_b_a : dictrionary of asymmetric accomodation from speaker b to speaker a\n",
    "            C_a_b : dictionary of asymmetric accomodation from speaker a to speaker b\n",
    "            LSM : dictionary of symmetric accomodation between both speakers\n",
    "            mean_C_b_a : average accomodation from b towards a across valid markers\n",
    "            mean_C_a_b : average accomodation from a towards b across valid markers\n",
    "            mean_LSM : average of symmetric accommodation\n",
    "            valid_markers : list of valid markers\n",
    "    '''\n",
    "    # ~~~~~~~~~~~ VARIABLES ~~~~~~~~~~~\n",
    "    #     > pairs - dictionary containing interlocution information\n",
    "    #     > raw_b_a - number of style markers used in all responses from b to a\n",
    "    #     > raw_a_b - number of style markers used in all responses from a to b\n",
    "    #     > baseline_b_a - probability of style markers in b's response to a\n",
    "    #     > baseline_a_b - probability of style markers in a's response to b\n",
    "    #     > elicit_b_a - probability of style markers in b's response to a given a exhibited the same marker\n",
    "    #     > elicit_a_b - probability of style markers in a's response to a given b exhibited the same marker\n",
    "    \n",
    "    speakers, utts = get_speaker_utt_lists(conv)\n",
    "    pairs = get_pairs(speakers, utts)\n",
    "\n",
    "    # Get markers from speaker a to b\n",
    "    # Note the order of a_b switched to b_a here. This is to be consistent with\n",
    "    # the notation of C(b,a) indicating the coordination of b to a\n",
    "    elicit_b_a = initialize_dict()\n",
    "    baseline_b_a = initialize_dict()\n",
    "    for a_b in pairs['a_b']:\n",
    "        u_a  = corpus.get_utterance(a_b[0])\n",
    "        u_b = corpus.get_utterance(a_b[1])\n",
    "        m_u_a = get_style_markers(u_a)\n",
    "        m_u_b = get_style_markers(u_b)\n",
    "        for k in m_u_a:\n",
    "            if m_u_a[k]:\n",
    "                if m_u_a[k] == m_u_b[k]:  # If b responded to a with same style marker\n",
    "                    elicit_b_a[k] += 1\n",
    "            baseline_b_a[k] += m_u_b[k] # b's response contains m regardless of a's prompt\n",
    "    \n",
    "    # Get markers from speaker b to a\n",
    "    elicit_a_b = initialize_dict()\n",
    "    baseline_a_b = initialize_dict()\n",
    "    for b_a in pairs['b_a']:\n",
    "        u_b  = corpus.get_utterance(b_a[0])\n",
    "        u_a = corpus.get_utterance(b_a[1])\n",
    "        m_u_a = get_style_markers(u_a)\n",
    "        m_u_b = get_style_markers(u_b)\n",
    "        for k in m_u_b:  \n",
    "            if m_u_b[k]:\n",
    "                if m_u_b[k] == m_u_a[k]:  # If a responded to b with same style marker\n",
    "                    elicit_a_b[k] += 1\n",
    "            baseline_a_b[k] += m_u_a[k] # If a's response contains m regardless of b's prompt\n",
    "    \n",
    "    \n",
    "    # Convert to probabilities, preserving raw baselines for LSM calculation\n",
    "    raw_b_a = baseline_b_a.copy()\n",
    "    raw_a_b = baseline_a_b.copy()\n",
    "    num_response_b_a = len(pairs['a_b'])  # Number of responses from b to a\n",
    "    num_response_a_b = len(pairs['b_a'])  # Number of responses from a to b\n",
    "    for k in elicit_a_b.keys():  # Could be any dictionary, they all have the same keys\n",
    "        elicit_b_a[k] = elicit_b_a[k] / num_response_b_a \n",
    "        baseline_b_a[k] = baseline_b_a[k] / num_response_b_a\n",
    "        elicit_a_b[k] = elicit_a_b[k] / num_response_a_b\n",
    "        baseline_a_b[k] = baseline_a_b[k] / num_response_a_b\n",
    "\n",
    "    # Determine asymmetric and symmetric accomodation\n",
    "    C_b_a = initialize_dict() # Accomodation of b towards a\n",
    "    C_a_b = initialize_dict() # Accomodation of a towards b\n",
    "    LSM = initialize_dict()\n",
    "    for k in C_b_a.keys():\n",
    "        if baseline_b_a[k] and baseline_a_b[k]:  # If a and b both exhibited marker m at some point\n",
    "            C_b_a[k] = baseline_b_a[k] - elicit_b_a[k]\n",
    "            C_a_b[k] = baseline_a_b[k] - elicit_a_b[k]\n",
    "            LSM[k] = 1 - abs(raw_a_b[k] - raw_b_a[k]) / (raw_a_b[k] + raw_b_a[k] + 0.0001)\n",
    "        else:                                    # Else, the metric is undefined for marker m\n",
    "            C_b_a[k] = None  # Set to None if there is no data\n",
    "            C_a_b[k] = None\n",
    "            LSM[k] = None\n",
    "\n",
    "    # Get averages across asymmetric measure\n",
    "    valid_markers = []\n",
    "    mean_C_b_a = 0\n",
    "    mean_C_a_b = 0\n",
    "    mean_LSM = 0\n",
    "    for k in C_b_a.keys():\n",
    "        if C_b_a[k] is not None:\n",
    "            mean_C_b_a += C_b_a[k]\n",
    "            mean_C_a_b += C_a_b[k]\n",
    "            mean_LSM += LSM[k]\n",
    "            valid_markers.append(k)\n",
    "    if valid_markers:\n",
    "        mean_C_b_a /= len(valid_markers)\n",
    "        mean_C_a_b /= len(valid_markers)\n",
    "        mean_LSM /= len(valid_markers)\n",
    "\n",
    "    # Construct dictionary to return\n",
    "    C = {\n",
    "        'convID' : conv.id,\n",
    "        'a' : pairs['a'],\n",
    "        'b' : pairs['b'],\n",
    "        'num_response_b_a' : len(pairs['b_a']),\n",
    "        'num_response_a_b' : len(pairs['a_b']),\n",
    "        'C_b_a' : C_b_a,\n",
    "        'C_a_b' : C_a_b,\n",
    "        'LSM' : LSM,\n",
    "        'mean_C_b_a' : mean_C_b_a,\n",
    "        'mean_C_a_b' : mean_C_a_b,\n",
    "        'mean_LSM' : mean_LSM,\n",
    "        'valid_markers' : valid_markers\n",
    "    } \n",
    "\n",
    "    if print_output:\n",
    "        print('pairs: ', pairs)\n",
    "        print('\\nraw_b_a: ', raw_b_a)\n",
    "        print('raw_a_b: ', raw_a_b)\n",
    "        print('\\nelicit_b_a: ', elicit_b_a)\n",
    "        print('elicit_a_b: ', elicit_a_b)\n",
    "        print('\\nbaseline_b_a: ', baseline_b_a)\n",
    "        print('baseline_a_b: ', baseline_a_b)\n",
    "        print('\\nC_b_a: ', C_b_a)\n",
    "        print('C_a_b: ', C_a_b)\n",
    "        print('\\nLSM: ', LSM)\n",
    "        print('\\nmean_C_b_a: ', mean_C_b_a)\n",
    "        print('mean_C_a_b: ', mean_C_a_b)\n",
    "        \n",
    "    return C\n",
    "\n",
    "\n",
    "conv = corpus.get_conversation(valid_conv_ids[20])\n",
    "C = measure_coordination(conv, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convID : 162766001.2409.2409\n",
      "a : The undertow\n",
      "b : Indubitably\n",
      "num_response_b_a : 2\n",
      "num_response_a_b : 3\n",
      "\n",
      "~~ C_b_a ~~\n",
      "     ppron : 0.00\n",
      "     ipron : 0.00\n",
      "     article : 0.00\n",
      "     conj : 0.00\n",
      "     prep : 0.00\n",
      "     auxverb : 0.00\n",
      "     adverb : 0.00\n",
      "     negate : 0.00\n",
      "     quant : None\n",
      "\n",
      "~~ C_a_b ~~\n",
      "     ppron : 0.50\n",
      "     ipron : 0.00\n",
      "     article : 0.00\n",
      "     conj : 0.50\n",
      "     prep : 0.00\n",
      "     auxverb : 0.00\n",
      "     adverb : 0.00\n",
      "     negate : 0.50\n",
      "     quant : None\n",
      "\n",
      "~~ LSM ~~\n",
      "     ppron : 0.67\n",
      "     ipron : 1.00\n",
      "     article : 0.80\n",
      "     conj : 1.00\n",
      "     prep : 0.67\n",
      "     auxverb : 1.00\n",
      "     adverb : 0.80\n",
      "     negate : 0.67\n",
      "     quant : None\n",
      "\n",
      "\n",
      "mean_C_b_a : 0.0\n",
      "mean_C_a_b : 0.1875\n",
      "mean_LSM : 0.8250051665077829\n",
      "valid_markers : ['ppron', 'ipron', 'article', 'conj', 'prep', 'auxverb', 'adverb', 'negate']\n"
     ]
    }
   ],
   "source": [
    "def print_coordination(C):\n",
    "    '''\n",
    "    Prints a coordination dictionary (output from measure_coordination) legibly\n",
    "    '''\n",
    "    for k in C.keys():\n",
    "        if isinstance(C[k], dict):\n",
    "            print('\\n~~ {} ~~'.format(k))\n",
    "            for m in C[k].keys():\n",
    "                if C[k][m] is not None:\n",
    "                    print('     {} : {:.2f}'.format(m, C[k][m]))\n",
    "                else:\n",
    "                    print('     {} : None'.format(m))\n",
    "            if k == 'LSM':\n",
    "                print('\\n')\n",
    "        else:\n",
    "            print('{} : {}'.format(k, C[k]))\n",
    "\n",
    "print_coordination(C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
