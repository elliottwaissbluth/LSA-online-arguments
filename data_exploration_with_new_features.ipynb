{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversations gone awry, Wikipedia version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /home/gaoag/.convokit/downloads/conversations-gone-awry-cmv-corpus\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download(\"conversations-gone-awry-cmv-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /home/gaoag/.convokit/downloads/conversations-gone-awry-corpus\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download('conversations-gone-awry-corpus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /home/gaoag/.convokit/downloads/conversations-gone-awry-corpus\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, download\n",
    "import spacy\n",
    "import pickle\n",
    "corpus = Corpus(filename=download('conversations-gone-awry-corpus'))\n",
    "\n",
    "# Load liwc_dic\n",
    "with open('liwc_dic.pkl', 'rb') as handle:\n",
    "    liwc_dic = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 8069\n",
      "Number of Utterances: 30021\n",
      "Number of Conversations: 4188\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dposse\n"
     ]
    }
   ],
   "source": [
    "utt = corpus.random_utterance()\n",
    "print(utt.speaker.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation('id': '103071967.14294.14294', 'utterances': ['103071967.14294.14294', '103071967.14308.14294', '103085389.14715.14715', '103087243.14786.14786'], 'meta': {'page_title': 'Template talk:Summary of casualties of the Iraq War', 'page_id': 2877154, 'pair_id': '25213805.0.0', 'conversation_has_personal_attack': True, 'verified': True, 'pair_verified': True, 'annotation_year': '2018', 'split': 'val'})\n"
     ]
    }
   ],
   "source": [
    "convo = corpus.random_conversation()\n",
    "print(convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['103071967.14294.14294', '103071967.14308.14294', '103085389.14715.14715', '103087243.14786.14786']\n",
      "['Seigfried4220', 'Seigfried4220', 'Timeshifter', 'Seigfried4220']\n"
     ]
    }
   ],
   "source": [
    "paths = convo.get_longest_paths()\n",
    "for path in paths:\n",
    "    print([utt.id for utt in path])\n",
    "    print([utt.get_speaker().id for utt in path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utterance features\n",
    "- **id**: index of the utterance\n",
    "- **conversation_id**: id of the first utterance in the converstaion this utterance belongs to\n",
    "- **reply-to**: index of the utterance to which this utterance replies to (None if not a reply)\n",
    "- **speaker**: the speaker who authored the utterance\n",
    "- **timestamp**: timestamp of utterance\n",
    "- **text**: textual content of the utterance\n",
    "- **meta**: metadata for each utterance\n",
    "    - **is_section_header**: whether the utterance is a conversation \"title\" or \"subject\" (if true, the utterance should be ignored)\n",
    "    - **comment_has_personal_attack**: whether this comment was judged by 3 crowdsourced annotators to contain a personal comment_has_personal_attack\n",
    "    - **parsed**: SpaCy parsed version of the utterance text\n",
    "        - **rt**: ??\n",
    "        - **toks**: List of parsed tokens\n",
    "            - **tok**: the token (word, punctuation, etc.)\n",
    "            - **tag**: Detailed part of speech tag\n",
    "            - **dep**: syntactic dependency, i.e. the relation between the tokens\n",
    "            - **up**: list related to dn, not sure how\n",
    "            - **dn**: list related to up, not sure how"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation features\n",
    "\n",
    "- **id**: id of the conversation\n",
    "- **utterances**: ids of utterances in the conversation (in order I presume)\n",
    "- **meta**: conversation metadata\n",
    "    - **page_title**: title of page under which conversation is occurring\n",
    "    - **page_id**: unique numerical id of the talk page\n",
    "    - **pair_id**: the id of the conversation that this comment's conversation is paired with\n",
    "    - **conversation_has_personal_attack**: whether any comment in this comment's conversation contains a personal attack\n",
    "    - **verified**: whether the personal attack label has been verified by an internal annotator\n",
    "    - **pair_verified**: whether the personal attack label has been double checked by the internal annotator\n",
    "    - **annotation_year**: self explanatory\n",
    "    - **split**: (train, test, or val) whether this conversation was used as train, test, or val in \"Trouble on the Horizon\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to find the conversations that are easy to analyze, i.e. have a structure like (a -> b -> a -> b -> ...). detect_interlocution should reveal those conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(634/4188) 15.1% conversations valid\n"
     ]
    }
   ],
   "source": [
    "# We want to consider conversations with a call-reply structure between two speakers, having at least five utterances\n",
    "def detect_interlocution(conv, min_utts, print=False):\n",
    "    '''\n",
    "    Finds whether the conversation has a call-reply structure between two speakers with at least min_utts utterances\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > conv - entire conversation object\n",
    "        > min_utts - the minimum number of utterances that constitute a valid conversation\n",
    "        > print - whether or not to print why the conversation was rejected\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > bool representing whether or not the conversation's longest path has the aforementioned structure\n",
    "    '''\n",
    "    # At the moment, only considering first longest path if there are multiple # TODO: Add functionality to examine all paths\n",
    "    try:\n",
    "        longest_path = conv.get_longest_paths()[0]\n",
    "    except ValueError as v:\n",
    "        if print:\n",
    "            print(v)\n",
    "            print('skipping...')\n",
    "        return False\n",
    "    \n",
    "    if len(longest_path) < min_utts:\n",
    "        if print:\n",
    "            print('Less than {} utterances in conversation\\nskipping...'.format(min_utts))\n",
    "        return False\n",
    "\n",
    "    speakers = []\n",
    "\n",
    "    for utt in longest_path:\n",
    "        speakers.append(utt.get_speaker().id)\n",
    "    \n",
    "    if len(set(speakers)) > 2:\n",
    "        if print:\n",
    "            print('More than 2 speakers in conversation\\nskipping...')\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Determine number of valid conversations\n",
    "num_valid = 0\n",
    "valid_conv_ids = []  # Will hold IDs of all valid converations\n",
    "for conv in corpus.iter_conversations():\n",
    "    if detect_interlocution(conv, 5):\n",
    "        valid_conv_ids.append(conv.id)\n",
    "        num_valid += 1\n",
    "\n",
    "print('({}/{}) {:.1f}% conversations valid'.format(num_valid, len(corpus.get_conversation_ids()), num_valid*100/len(corpus.get_conversation_ids())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get a single conversation from list of valid IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation({'obj_type': 'conversation', 'meta': {'page_title': 'User talk:AngryParsley', 'page_id': 1282978, 'pair_id': '12941035.584.584', 'conversation_has_personal_attack': False, 'verified': True, 'pair_verified': True, 'annotation_year': '2018', 'split': 'train'}, 'vectors': [], 'tree': <convokit.model.utteranceNode.UtteranceNode object at 0x7f56aa75a940>, 'owner': <convokit.model.corpus.Corpus object at 0x7f570d3d7370>, 'id': '12451425.436.436'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_conversation(valid_conv_ids[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of valid conversations, let's find a way to isolate the utterances in a way that we can easily analyze for style accommodation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 'Mike Garcia',\n",
       " 'b': '66.36.136.123',\n",
       " 'a_b': [('15832939.3151.3151', '15833000.3256.3256')],\n",
       " 'b_a': [('15832773.3035.3019', '15832939.3151.3151'),\n",
       "  ('15833000.3256.3256', '15833036.3275.3275')]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_speaker_utt_lists(conv):\n",
    "    '''\n",
    "    Generates lists of speaker IDs corresponding to utterances in conv, and gets utterances\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > conv - entire conversation object\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > speakers - list of speakers corresponding to each utterance\n",
    "        > utts - list of utterances from conversation\n",
    "    '''\n",
    "    # Recall, we use the first index longest path in our list of valid conversations\n",
    "    longest_path = conv.get_longest_paths()[0]\n",
    "\n",
    "    speakers = []\n",
    "    utts = []\n",
    "\n",
    "    for utt in longest_path:\n",
    "        utts.append(utt.id)\n",
    "        speakers.append(utt.get_speaker().id)\n",
    "\n",
    "    return speakers, utts\n",
    "\n",
    "def get_pairs(speakers, utts):\n",
    "    '''\n",
    "    Generates a dictionary of pairs of utterances, each pair representing a back and forth interaction\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > speakers - list of speakers corresponding to each utterance\n",
    "        > utts - list of utterances from conversation\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > pairs - dictionary with the following structure\n",
    "            a : ID of speaker a\n",
    "            b : ID of speaker b\n",
    "            a_b : [(tuple of 2 utterance IDs, first being from speaker a and second from speaker b), (...), ...]\n",
    "            b_a : [(tuple of 2 utterance IDs, first being from speaker b and second from speaker a), (...), ...]\n",
    "    '''\n",
    "    # TODO: Account for instances where a speaker speaks multiple times in a row. Combine those into a list of utteranes within the tuples\n",
    "    pairs = {\n",
    "        'a' : list(set(speakers))[0],\n",
    "        'b' : list(set(speakers))[1],\n",
    "        'a_b' : [],\n",
    "        'b_a' : []\n",
    "    }\n",
    "\n",
    "    # We'll say speaker a is the first speaker, and speaker b is the second.False\n",
    "    speaker_shift = [1 if speakers[i] != speakers[i-1] else 0 for i in range(1, len(speakers))]\n",
    "    speaker_shift.insert(0,0) # Prepend 0 (first utterance isn't a response)\n",
    "\n",
    "    for i in range(1, len(speakers)):\n",
    "        if speakers[i] == pairs['b'] and speakers[i-1] == pairs['a']:\n",
    "            pairs['a_b'].append((utts[i-1], utts[i]))\n",
    "        elif speakers[i] == pairs['a'] and speakers[i-1] == pairs['b']:\n",
    "            pairs['b_a'].append((utts[i-1], utts[i]))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "speakers, utts = get_speaker_utt_lists(corpus.get_conversation(valid_conv_ids[3]))\n",
    "get_pairs(speakers, utts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure style accommodation we have to measure the style markers in each utterance. That's what this following function is for.\n",
    "\n",
    "see https://spacy.io/models/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = corpus.get_conversation(valid_conv_ids[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation({'obj_type': 'conversation', 'meta': {'page_title': 'Talk:Mezmerize (album)', 'page_id': 1219116, 'pair_id': '15835089.3408.3408', 'conversation_has_personal_attack': True, 'verified': True, 'pair_verified': True, 'annotation_year': '2018', 'split': 'test'}, 'vectors': [], 'tree': <convokit.model.utteranceNode.UtteranceNode object at 0x7f563eeb24f0>, 'owner': <convokit.model.corpus.Corpus object at 0x7f565789bb80>, 'id': '15832773.3019.3019'})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conjunction, subordinating or preposition'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('IN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy and ARK tags\n",
    "\n",
    "\n",
    "| ARK tag | spaCy tag \t| our tag \t| intended definition \t| actual                                   \t|\n",
    "|-------- |-----------\t|---------\t|---------------------\t|------------------------------------------\t|\n",
    "| O       | PRP       \t| ppron   \t| personal pronoun    \t| personal pronoun                         \t|\n",
    "|         |           \t| ipron   \t| impersonal pronoun  \t|                                          \t|\n",
    "|         |           \t| article \t| article             \t|                                          \t|\n",
    "| &       | CC        \t| conj    \t| conjunction         \t| coordinating conjunction                 \t|\n",
    "| P       | IN        \t| prep    \t| preposition         \t| subordinating or preposition conjunction \t|\n",
    "| V       | MD        \t| auxverb \t| auxiliary verb      \t| modal auxiliary verb                     \t|\n",
    "| R       | RB        \t| adverb  \t| common adverb       \t| adverb                                   \t|\n",
    "|         |           \t| negate  \t| negation            \t|                                          \t|\n",
    "|         |           \t| quant   \t| quantifier          \t|                                          \t|\n",
    "|N        |             |           | noun                  |                                           |  \n",
    "|^        |             |           | proper noun           |                                           |\n",
    "|S        |             |           | nominal+possessive    |                                           |\n",
    "|Z        |             |           | proper noun+posessive |                                           |\n",
    "|L        |             |           | nominal+verbal        |                                           |\n",
    "|M        |             |           | proper noun+verbal    |                                           |\n",
    "|A        |             |           | adjective             |                                           |\n",
    "|!        |             |           | interjection          | use                                       |\n",
    "|D        |             |           | determiner            |                                           |\n",
    "|T        |             |           | verb particle         |                                           |\n",
    "|X        |             |           | existential \"there\"   |                                           |\n",
    "|Y        |             |           | x+verbal              |                                           |\n",
    "|#        |             |           | hashtag               |                                           |\n",
    "|@        |             |           | @ mention             |                                           |\n",
    "|~        |             |           | discourse marker      | use (can't figure it out though)          |\n",
    "|U        |             |           | URL or email          |                                           |\n",
    "|E        |             |           | emoticon              | use                                       |\n",
    "|$        |             |           | numeral               |                                           |\n",
    "|,        |             |           | punctuation           | use                                       |\n",
    "|G        |             |           | garbage/other         |                                           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /home/gaoag/.convokit/downloads/reddit-corpus-small\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download(\"reddit-corpus-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = corpus.get_conversation('9c0sn1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import subprocess\n",
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def get_style_markers(utt):\n",
    "    '''\n",
    "    Returns a dictionary containing the number of style markers in an utterance\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > utt - a single utterance\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > m - dictionary with the following key value pairs\n",
    "            ppron : # personal pronouns\n",
    "            ipron : # impersonal pronouns\n",
    "            article : # articles\n",
    "            conj : # conjunctions\n",
    "            prep : # prepositions\n",
    "            auxverb : # auxiliary verbs\n",
    "            adverb : # common adverbs\n",
    "            negate : # negations\n",
    "            quant : # quantifiers\n",
    "    '''\n",
    "    m = {\n",
    "        'ppron' : 0,\n",
    "        'ipron' : 0,\n",
    "        'article' : 0,\n",
    "        'conj' : 0,\n",
    "        'prep' : 0,\n",
    "        'auxverb' : 0,\n",
    "        'adverb' : 0,\n",
    "        'negate' : 0,\n",
    "        'quant' : 0\n",
    "    }\n",
    "\n",
    "    # Tokenize text\n",
    "    text = utt.text.lower().split()\n",
    "\n",
    "    # Analyze using the LIWC keys\n",
    "    for word in text:\n",
    "        if word in liwc_dic.keys():\n",
    "            m[liwc_dic[word]] += 1\n",
    "    \n",
    "    # Convert to boolean\n",
    "    for k in m.keys():\n",
    "        if m[k]:\n",
    "            m[k] = 1\n",
    "\n",
    "    return m\n",
    "\n",
    "def get_style_markers_extended(utt):\n",
    "    '''\n",
    "    Returns a dictionary containing the number of style markers in an utterance\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > utt - a single utterance\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > m - dictionary with the following key value pairs - (outlined in the table above)\n",
    "        should probably select for ones that are somewhat indicative of style - G and @, for instance, probably don't matter\n",
    "            \n",
    "    '''\n",
    "    m = {\n",
    "#         'N' : 0,\n",
    "        'O' : 0,\n",
    "#         '^' : 0,\n",
    "#         'S' : 0,\n",
    "#         'Z' : 0,\n",
    "        'V' : 0,\n",
    "#         'L' : 0,\n",
    "#         'M' : 0,\n",
    "#         'A' : 0,\n",
    "        'R' : 0,\n",
    "        '!' : 0,\n",
    "#         'D' : 0,\n",
    "        'P' : 0,\n",
    "        '&' : 0,\n",
    "#         'T' : 0,\n",
    "#         'X' : 0,\n",
    "#         'Y' : 0,\n",
    "#         '#' : 0,\n",
    "#         '@' : 0,\n",
    "#         '~' : 0,\n",
    "#         'U' : 0,\n",
    "        'E' : 0,\n",
    "#         '$' : 0,\n",
    "        ',' : 0,\n",
    "#         'G' : 0,\n",
    "        'ipron' : 0,\n",
    "        'article' : 0,\n",
    "        'negate' : 0,\n",
    "        'quant' : 0\n",
    "    }\n",
    "    \n",
    "    text_demojied_with_marker = emoji_pattern.sub('wkkemojification', utt.text)\n",
    "    \n",
    "   \n",
    "    # save utt text to file in the appropriate format\n",
    "    temp_file_obj = open(\"tempinput.txt\", 'w')\n",
    "    temp_file_obj.write(text_demojied_with_marker)\n",
    "    temp_file_obj.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # subprocess the file to the java jar; pipe output to a script that returns a wordXPOS dict \n",
    "    proc = subprocess.Popen(['../ark-tweet-nlp/runTagger.sh', './tempinput.txt'], stdout=subprocess.PIPE)\n",
    "#     output = subprocess.check_output(['~/nlp/ark-tweet-nlp/scripts/'], stdin = runtagger_call.stdout)\n",
    "    while True:\n",
    "        line = proc.stdout.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        tokens, tags, conf, orig = [l.decode(\"utf-8\") for l in line.split(b'\\t')]\n",
    "        for tok, tag in zip(tokens, tags):\n",
    "            if tag in m.keys():\n",
    "                m[tag] += 1\n",
    "            if tok in liwc_dic.keys() and liwc_dic[tok] in m.keys():\n",
    "                m[liwc_dic[tok]] += 1\n",
    "                \n",
    "    # Convert to boolean\n",
    "    for k in m.keys():\n",
    "        if m[k]:\n",
    "            m[k] = 1\n",
    "\n",
    "    return m\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dict():\n",
    "    return {\n",
    "        'ppron' : 0,\n",
    "        'ipron' : 0,\n",
    "        'article' : 0,\n",
    "        'conj' : 0,\n",
    "        'prep' : 0,\n",
    "        'auxverb' : 0,\n",
    "        'adverb' : 0,\n",
    "        'negate' : 0,\n",
    "        'quant' : 0\n",
    "    }\n",
    "\n",
    "\n",
    "def initialize_dict_extended():\n",
    "    return  {\n",
    "#         'N' : 0,\n",
    "        'O' : 0,\n",
    "#         '^' : 0,\n",
    "#         'S' : 0,\n",
    "#         'Z' : 0,\n",
    "        'V' : 0,\n",
    "#         'L' : 0,\n",
    "#         'M' : 0,\n",
    "#         'A' : 0,\n",
    "        'R' : 0,\n",
    "        '!' : 0,\n",
    "#         'D' : 0,\n",
    "        'P' : 0,\n",
    "        '&' : 0,\n",
    "#         'T' : 0,\n",
    "#         'X' : 0,\n",
    "#         'Y' : 0,\n",
    "#         '#' : 0,\n",
    "#         '@' : 0,\n",
    "#         '~' : 0,\n",
    "#         'U' : 0,\n",
    "        'E' : 0,\n",
    "#         '$' : 0,\n",
    "        ',' : 0,\n",
    "#         'G' : 0,\n",
    "        'ipron' : 0,\n",
    "        'article' : 0,\n",
    "        'negate' : 0,\n",
    "        'quant' : 0\n",
    "    }\n",
    "    \n",
    "    \n",
    "def measure_coordination(conv, print_output=False, extended=False):\n",
    "    '''\n",
    "    Assumes the converation will only have two speakers\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > conv - entire conversation object\n",
    "        > print_output - whether to print medial variables\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > C - dictionary with following key value pairs\n",
    "            convID : ID of conversation\n",
    "            a : ID of speaker a\n",
    "            b : ID of speaker b\n",
    "            num_response_b_a : number of responses from b to a\n",
    "            num_response_a_b : number of responses from a to b\n",
    "            C_b_a : dictrionary of asymmetric accomodation from speaker b to speaker a\n",
    "            C_a_b : dictionary of asymmetric accomodation from speaker a to speaker b\n",
    "            LSM : dictionary of symmetric accomodation between both speakers\n",
    "            mean_C_b_a : average accomodation from b towards a across valid markers\n",
    "            mean_C_a_b : average accomodation from a towards b across valid markers\n",
    "            mean_LSM : average of symmetric accommodation\n",
    "            valid_markers : list of valid markers\n",
    "    '''\n",
    "    # ~~~~~~~~~~~ VARIABLES ~~~~~~~~~~~\n",
    "    #     > pairs - dictionary containing interlocution information\n",
    "    #     > raw_b_a - number of style markers used in all responses from b to a\n",
    "    #     > raw_a_b - number of style markers used in all responses from a to b\n",
    "    #     > baseline_b_a - probability of style markers in b's response to a\n",
    "    #     > baseline_a_b - probability of style markers in a's response to b\n",
    "    #     > elicit_b_a - probability of style markers in b's response to a given a exhibited the same marker\n",
    "    #     > elicit_a_b - probability of style markers in a's response to a given b exhibited the same marker\n",
    "    \n",
    "    speakers, utts = get_speaker_utt_lists(conv)\n",
    "    pairs = get_pairs(speakers, utts)\n",
    "\n",
    "    # Get markers from speaker a to b\n",
    "    # Note the order of a_b switched to b_a here. This is to be consistent with\n",
    "    # the notation of C(b,a) indicating the coordination of b to a\n",
    "    if extended:\n",
    "        elicit_b_a = initialize_dict_extended()\n",
    "        baseline_b_a = initialize_dict_extended()\n",
    "    else:\n",
    "        elicit_b_a = initialize_dict()\n",
    "        baseline_b_a = initialize_dict()\n",
    "    for a_b in pairs['a_b']:\n",
    "        u_a  = corpus.get_utterance(a_b[0])\n",
    "        u_b = corpus.get_utterance(a_b[1])\n",
    "        if extended:\n",
    "            m_u_a = get_style_markers_extended(u_a)\n",
    "            m_u_b = get_style_markers_extended(u_b)\n",
    "        else:\n",
    "            m_u_a = get_style_markers(u_a)\n",
    "            m_u_b = get_style_markers(u_b)\n",
    "        for k in m_u_a:\n",
    "            if m_u_a[k]:\n",
    "                if m_u_a[k] == m_u_b[k]:  # If b responded to a with same style marker\n",
    "                    elicit_b_a[k] += 1\n",
    "            baseline_b_a[k] += m_u_b[k] # b's response contains m regardless of a's prompt\n",
    "    \n",
    "    # Get markers from speaker b to a\n",
    "    \n",
    "    if extended:\n",
    "        elicit_a_b = initialize_dict_extended()\n",
    "        baseline_a_b = initialize_dict_extended()\n",
    "    else:\n",
    "        elicit_a_b = initialize_dict()\n",
    "        baseline_a_b = initialize_dict()\n",
    "        \n",
    "    for b_a in pairs['b_a']:\n",
    "        u_b  = corpus.get_utterance(b_a[0])\n",
    "        u_a = corpus.get_utterance(b_a[1])\n",
    "        if extended:\n",
    "            m_u_a = get_style_markers_extended(u_a)\n",
    "            m_u_b = get_style_markers_extended(u_b)\n",
    "        else:\n",
    "            m_u_a = get_style_markers(u_a)\n",
    "            m_u_b = get_style_markers(u_b)\n",
    "        for k in m_u_b:  \n",
    "            if m_u_b[k]:\n",
    "                if m_u_b[k] == m_u_a[k]:  # If a responded to b with same style marker\n",
    "                    elicit_a_b[k] += 1\n",
    "            baseline_a_b[k] += m_u_a[k] # If a's response contains m regardless of b's prompt\n",
    "    \n",
    "    \n",
    "    # Convert to probabilities, preserving raw baselines for LSM calculation\n",
    "    raw_b_a = baseline_b_a.copy()\n",
    "    raw_a_b = baseline_a_b.copy()\n",
    "    num_response_b_a = len(pairs['a_b'])  # Number of responses from b to a\n",
    "    num_response_a_b = len(pairs['b_a'])  # Number of responses from a to b\n",
    "    for k in elicit_a_b.keys():  # Could be any dictionary, they all have the same keys\n",
    "        elicit_b_a[k] = elicit_b_a[k] / num_response_b_a \n",
    "        baseline_b_a[k] = baseline_b_a[k] / num_response_b_a\n",
    "        elicit_a_b[k] = elicit_a_b[k] / num_response_a_b\n",
    "        baseline_a_b[k] = baseline_a_b[k] / num_response_a_b\n",
    "\n",
    "    # Determine asymmetric and symmetric accomodation\n",
    "    if extended:\n",
    "        C_b_a = initialize_dict_extended()\n",
    "        C_a_b = initialize_dict_extended()\n",
    "        LSM = initialize_dict_extended()\n",
    "    else:\n",
    "        C_b_a = initialize_dict() # Accomodation of b towards a\n",
    "        C_a_b = initialize_dict() # Accomodation of a towards b\n",
    "        LSM = initialize_dict()\n",
    "    for k in C_b_a.keys():\n",
    "        if baseline_b_a[k] and baseline_a_b[k]:  # If a and b both exhibited marker m at some point\n",
    "            C_b_a[k] = baseline_b_a[k] - elicit_b_a[k]\n",
    "            C_a_b[k] = baseline_a_b[k] - elicit_a_b[k]\n",
    "            LSM[k] = 1 - abs(raw_a_b[k] - raw_b_a[k]) / (raw_a_b[k] + raw_b_a[k] + 0.0001)\n",
    "        else:                                    # Else, the metric is undefined for marker m\n",
    "            C_b_a[k] = None  # Set to None if there is no data\n",
    "            C_a_b[k] = None\n",
    "            LSM[k] = None\n",
    "\n",
    "    # Get averages across asymmetric measure\n",
    "    valid_markers = []\n",
    "    mean_C_b_a = 0\n",
    "    mean_C_a_b = 0\n",
    "    mean_LSM = 0\n",
    "    for k in C_b_a.keys():\n",
    "        if C_b_a[k] is not None:\n",
    "            mean_C_b_a += C_b_a[k]\n",
    "            mean_C_a_b += C_a_b[k]\n",
    "            mean_LSM += LSM[k]\n",
    "            valid_markers.append(k)\n",
    "    if valid_markers:\n",
    "        mean_C_b_a /= len(valid_markers)\n",
    "        mean_C_a_b /= len(valid_markers)\n",
    "        mean_LSM /= len(valid_markers)\n",
    "\n",
    "    # Construct dictionary to return\n",
    "    C = {\n",
    "        'convID' : conv.id,\n",
    "        'a' : pairs['a'],\n",
    "        'b' : pairs['b'],\n",
    "        'num_response_b_a' : len(pairs['b_a']),\n",
    "        'num_response_a_b' : len(pairs['a_b']),\n",
    "        'C_b_a' : C_b_a,\n",
    "        'C_a_b' : C_a_b,\n",
    "        'LSM' : LSM,\n",
    "        'mean_C_b_a' : mean_C_b_a,\n",
    "        'mean_C_a_b' : mean_C_a_b,\n",
    "        'mean_LSM' : mean_LSM,\n",
    "        'valid_markers' : valid_markers\n",
    "    } \n",
    "\n",
    "    if print_output:\n",
    "        print('pairs: ', pairs)\n",
    "        print('\\nraw_b_a: ', raw_b_a)\n",
    "        print('raw_a_b: ', raw_a_b)\n",
    "        print('\\nelicit_b_a: ', elicit_b_a)\n",
    "        print('elicit_a_b: ', elicit_a_b)\n",
    "        print('\\nbaseline_b_a: ', baseline_b_a)\n",
    "        print('baseline_a_b: ', baseline_a_b)\n",
    "        print('\\nC_b_a: ', C_b_a)\n",
    "        print('C_a_b: ', C_a_b)\n",
    "        print('\\nLSM: ', LSM)\n",
    "        print('\\nmean_C_b_a: ', mean_C_b_a)\n",
    "        print('mean_C_a_b: ', mean_C_a_b)\n",
    "        \n",
    "    return C\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "15\n",
      "30\n",
      "45\n",
      "60\n",
      "75\n",
      "90\n",
      "105\n",
      "120\n",
      "135\n",
      "150\n",
      "<class 'Exception'> 143206656.10.10\n",
      "165\n",
      "<class 'Exception'> 97524250.56095.56095\n",
      "180\n",
      "<class 'Exception'> 368853045.12992.12992\n",
      "195\n",
      "210\n",
      "<class 'Exception'> 724965073.205638.205638\n",
      "<class 'Exception'> 48973343.25345.25345\n",
      "<class 'Exception'> 709553734.4699.4699\n",
      "225\n",
      "240\n",
      "<class 'Exception'> 230840298.7670.7670\n",
      "255\n",
      "270\n",
      "285\n",
      "<class 'Exception'> 468953735.92455.92455\n",
      "300\n",
      "<class 'Exception'> 45970779.4829.4829\n",
      "315\n",
      "330\n",
      "345\n",
      "360\n",
      "<class 'Exception'> 139407294.103416.103416\n",
      "375\n",
      "390\n",
      "405\n",
      "<class 'Exception'> 593962760.54308.54308\n",
      "420\n",
      "435\n",
      "450\n",
      "465\n",
      "<class 'Exception'> 13521263.3340.3340\n",
      "480\n",
      "495\n",
      "510\n",
      "525\n",
      "540\n",
      "555\n",
      "570\n",
      "585\n",
      "<class 'Exception'> 598664341.8324.8324\n",
      "600\n",
      "615\n",
      "630\n"
     ]
    }
   ],
   "source": [
    "full_results = []\n",
    "\n",
    "for i, conv_id in enumerate(valid_conv_ids):\n",
    "    if i%15==0:\n",
    "        print(i)\n",
    "    conv = corpus.get_conversation(conv_id)\n",
    "    try:\n",
    "        results = measure_coordination(conv, print_output=False, extended=True)\n",
    "    except Exception as e:\n",
    "        print(Exception, conv_id)\n",
    "        continue\n",
    "    full_results.append(results)\n",
    "    \n",
    "pickle.dump(full_results, open(\"full_results_wikipedia.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /home/gaoag/.convokit/downloads/conversations-gone-awry-cmv-corpus\n",
      "(1224/6842) 17.9% conversations valid\n",
      "0\n",
      "15\n",
      "30\n",
      "<class 'Exception'> cxx5b28\n",
      "45\n",
      "60\n",
      "75\n",
      "90\n",
      "105\n",
      "120\n",
      "135\n",
      "150\n",
      "165\n",
      "180\n",
      "195\n",
      "210\n",
      "225\n",
      "240\n",
      "255\n",
      "270\n",
      "285\n",
      "300\n",
      "315\n",
      "330\n",
      "345\n",
      "360\n",
      "375\n",
      "390\n",
      "405\n",
      "420\n",
      "435\n",
      "450\n",
      "465\n",
      "480\n",
      "495\n",
      "510\n",
      "525\n",
      "540\n",
      "555\n",
      "570\n",
      "<class 'Exception'> dyw3uva\n",
      "585\n",
      "600\n",
      "615\n",
      "630\n",
      "645\n",
      "660\n",
      "675\n",
      "690\n",
      "705\n",
      "<class 'Exception'> e6hmhtc\n",
      "720\n",
      "735\n",
      "750\n",
      "765\n",
      "780\n",
      "795\n",
      "810\n",
      "825\n",
      "840\n",
      "855\n",
      "870\n",
      "885\n",
      "900\n",
      "915\n",
      "930\n",
      "945\n",
      "960\n",
      "975\n",
      "990\n",
      "1005\n",
      "<class 'Exception'> e7ycyr3\n",
      "<class 'Exception'> cy1fo2m\n",
      "1020\n",
      "1035\n",
      "1050\n",
      "1065\n",
      "1080\n",
      "1095\n",
      "1110\n",
      "1125\n",
      "1140\n",
      "<class 'Exception'> dnpzmco\n",
      "1155\n",
      "<class 'Exception'> dusor10\n",
      "1170\n",
      "1185\n",
      "<class 'Exception'> e1rufjv\n",
      "1200\n",
      "1215\n"
     ]
    }
   ],
   "source": [
    "q_full_results = []\n",
    "\n",
    "# Determine number of valid conversations\n",
    "corpus = Corpus(filename=download(\"conversations-gone-awry-cmv-corpus\"))\n",
    "num_valid = 0\n",
    "\n",
    "valid_conv_ids = []  # Will hold IDs of all valid converations\n",
    "for conv in corpus.iter_conversations():\n",
    "    if detect_interlocution(conv, 5):\n",
    "        valid_conv_ids.append(conv.id)\n",
    "        num_valid += 1\n",
    "\n",
    "print('({}/{}) {:.1f}% conversations valid'.format(num_valid, len(corpus.get_conversation_ids()), num_valid*100/len(corpus.get_conversation_ids())))\n",
    "\n",
    "\n",
    "for i, conv_id in enumerate(valid_conv_ids):\n",
    "    if i%15==0:\n",
    "        print(i)\n",
    "    conv = corpus.get_conversation(conv_id)\n",
    "    try:\n",
    "        results = measure_coordination(conv, print_output=False, extended=True)\n",
    "    except Exception as e:\n",
    "        print(Exception, conv_id)\n",
    "        continue\n",
    "    q_full_results.append(results)\n",
    "    \n",
    "pickle.dump(q_full_results, open(\"full_results_reddit.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convID : 162766001.2409.2409\n",
      "a : The undertow\n",
      "b : Indubitably\n",
      "num_response_b_a : 2\n",
      "num_response_a_b : 3\n",
      "\n",
      "~~ C_b_a ~~\n",
      "     ppron : 0.00\n",
      "     ipron : 0.00\n",
      "     article : 0.00\n",
      "     conj : 0.00\n",
      "     prep : 0.00\n",
      "     auxverb : 0.00\n",
      "     adverb : 0.00\n",
      "     negate : 0.00\n",
      "     quant : None\n",
      "\n",
      "~~ C_a_b ~~\n",
      "     ppron : 0.50\n",
      "     ipron : 0.00\n",
      "     article : 0.00\n",
      "     conj : 0.50\n",
      "     prep : 0.00\n",
      "     auxverb : 0.00\n",
      "     adverb : 0.00\n",
      "     negate : 0.50\n",
      "     quant : None\n",
      "\n",
      "~~ LSM ~~\n",
      "     ppron : 0.67\n",
      "     ipron : 1.00\n",
      "     article : 0.80\n",
      "     conj : 1.00\n",
      "     prep : 0.67\n",
      "     auxverb : 1.00\n",
      "     adverb : 0.80\n",
      "     negate : 0.67\n",
      "     quant : None\n",
      "\n",
      "\n",
      "mean_C_b_a : 0.0\n",
      "mean_C_a_b : 0.1875\n",
      "mean_LSM : 0.8250051665077829\n",
      "valid_markers : ['ppron', 'ipron', 'article', 'conj', 'prep', 'auxverb', 'adverb', 'negate']\n"
     ]
    }
   ],
   "source": [
    "def print_coordination(C):\n",
    "    '''\n",
    "    Prints a coordination dictionary (output from measure_coordination) legibly\n",
    "    '''\n",
    "    for k in C.keys():\n",
    "        if isinstance(C[k], dict):\n",
    "            print('\\n~~ {} ~~'.format(k))\n",
    "            for m in C[k].keys():\n",
    "                if C[k][m] is not None:\n",
    "                    print('     {} : {:.2f}'.format(m, C[k][m]))\n",
    "                else:\n",
    "                    print('     {} : None'.format(m))\n",
    "            if k == 'LSM':\n",
    "                print('\\n')\n",
    "        else:\n",
    "            print('{} : {}'.format(k, C[k]))\n",
    "\n",
    "print_coordination(C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
