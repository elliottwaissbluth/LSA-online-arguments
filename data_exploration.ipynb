{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd01131efc7635b497546d7e8fbc76ad9d1f9d5d5d7857bcde935d6feea39d08984",
   "display_name": "Python 3.8.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Conversations gone awry, Wikipedia version"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset already exists at C:\\Users\\ewais\\.convokit\\downloads\\conversations-gone-awry-corpus\n",
      "Dataset already exists at C:\\Users\\ewais\\.convokit\\downloads\\conversations-gone-awry-cmv-corpus\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, download\n",
    "import spacy\n",
    "import pickle\n",
    "corpus = Corpus(filename=download('conversations-gone-awry-corpus'))\n",
    "reddit_corpus = Corpus(filename=download('conversations-gone-awry-cmv-corpus'))\n",
    "\n",
    "# Load liwc_dic\n",
    "with open('liwc_dic.pkl', 'rb') as handle:\n",
    "    liwc_dic = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Conversation('id': '146743638.12652.12652', 'utterances': ['146743638.12652.12652', '146743638.12667.12652', '146842219.12874.12874', '146860774.13072.13072'], 'meta': {'page_title': 'User talk:2005', 'page_id': 1003212, 'pair_id': '143890867.11926.11926', 'conversation_has_personal_attack': False, 'verified': True, 'pair_verified': True, 'annotation_year': '2018', 'split': 'train'})\n"
     ]
    }
   ],
   "source": [
    "conv = corpus.get_conversation('146743638.12652.12652')\n",
    "print(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"I see what you saying I just read his pokerstars profile, It struck me when I saw the change because I remember him being called Bill when I watched the last season of high stakes poker, But you seem to have many more years experience in the Poker/Gambling world then I do(I'm still a bit of a newbie), so I wanted to check with you first. BTW as far as the WPT, I was thinking nine that made up final and 6 for the tv table, I read this article that say the WPT Final table is made up of 10 players, with the final six that make it on TV,  I just want to be sure that they are correct, when I update the players infobox stats, thanks ▪◦▪ \""
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "corpus.get_utterance('146860774.13072.13072').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Speakers: 8069\nNumber of Utterances: 30021\nNumber of Conversations: 4188\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SitNGo\n"
     ]
    }
   ],
   "source": [
    "utt = corpus.random_utterance()\n",
    "print(utt.speaker.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Conversation('id': '100646530.28762.28762', 'utterances': ['100646530.28800.28762', '100646530.28762.28762', '100783978.28875.28875', '103509566.29831.29831', '103517246.30257.30257', '103518966.30257.30257'], 'meta': {'page_title': 'Talk:1996 United States campaign finance controversy', 'page_id': 3079042, 'pair_id': '86036261.21021.21021', 'conversation_has_personal_attack': True, 'verified': False, 'pair_verified': False, 'annotation_year': '2019', 'split': 'test'})\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3d0c505b5671>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mutt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconvo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_utterance_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_utterance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mutt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "convo = corpus.random_conversation()\n",
    "print(convo)\n",
    "for utt in convo.get_utterance_ids():\n",
    "    print(corpus.get_utterance[utt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['100646530.28762.28762', '100646530.28800.28762', '100783978.28875.28875', '103509566.29831.29831', '103517246.30257.30257']\n['Mastgrr', 'Mastgrr', 'Jayzel68', 'Derex', 'Will Beback']\n[False, False, False, False, False]\n['100646530.28762.28762', '100646530.28800.28762', '100783978.28875.28875', '103509566.29831.29831', '103518966.30257.30257']\n['Mastgrr', 'Mastgrr', 'Jayzel68', 'Derex', 'Jayzel68']\n[False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "paths = convo.get_longest_paths()\n",
    "for path in paths:\n",
    "    print([utt.id for utt in path])\n",
    "    print([utt.get_speaker().id for utt in path])\n",
    "    print([corpus.get_utterance(utt.id).retrieve_meta('comment_has_personal_attack') for utt in path])"
   ]
  },
  {
   "source": [
    "## Utterance features\n",
    "- **id**: index of the utterance\n",
    "- **conversation_id**: id of the first utterance in the converstaion this utterance belongs to\n",
    "- **reply-to**: index of the utterance to which this utterance replies to (None if not a reply)\n",
    "- **speaker**: the speaker who authored the utterance\n",
    "- **timestamp**: timestamp of utterance\n",
    "- **text**: textual content of the utterance\n",
    "- **meta**: metadata for each utterance\n",
    "    - **is_section_header**: whether the utterance is a conversation \"title\" or \"subject\" (if true, the utterance should be ignored)\n",
    "    - **comment_has_personal_attack**: whether this comment was judged by 3 crowdsourced annotators to contain a personal comment_has_personal_attack\n",
    "    - **parsed**: SpaCy parsed version of the utterance text\n",
    "        - **rt**: ??\n",
    "        - **toks**: List of parsed tokens\n",
    "            - **tok**: the token (word, punctuation, etc.)\n",
    "            - **tag**: Detailed part of speech tag\n",
    "            - **dep**: syntactic dependency, i.e. the relation between the tokens\n",
    "            - **up**: list related to dn, not sure how\n",
    "            - **dn**: list related to up, not sure how"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Conversation features\n",
    "\n",
    "- **id**: id of the conversation\n",
    "- **utterances**: ids of utterances in the conversation (in order I presume)\n",
    "- **meta**: conversation metadata\n",
    "    - **page_title**: title of page under which conversation is occurring\n",
    "    - **page_id**: unique numerical id of the talk page\n",
    "    - **pair_id**: the id of the conversation that this comment's conversation is paired with\n",
    "    - **conversation_has_personal_attack**: whether any comment in this comment's conversation contains a personal attack\n",
    "    - **verified**: whether the personal attack label has been verified by an internal annotator\n",
    "    - **pair_verified**: whether the personal attack label has been double checked by the internal annotator\n",
    "    - **annotation_year**: self explanatory\n",
    "    - **split**: (train, test, or val) whether this conversation was used as train, test, or val in \"Trouble on the Horizon\"\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First, we want to find the conversations that are easy to analyze, i.e. have a structure like (a -> b -> a -> b -> ...). detect_interlocution should reveal those conversations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Not enough responses at index 15\n",
      "Not enough responses at index 32\n",
      "Not enough responses at index 94\n",
      "Not enough responses at index 95\n",
      "Not enough responses at index 119\n",
      "Not enough responses at index 155\n",
      "Not enough responses at index 157\n",
      "Not enough responses at index 159\n",
      "Not enough responses at index 164\n",
      "Not enough responses at index 174\n",
      "Not enough responses at index 181\n",
      "Not enough responses at index 184\n",
      "Not enough responses at index 192\n",
      "Not enough responses at index 197\n",
      "Error at index 210, removing\n",
      "Error at index 214, removing\n",
      "Error at index 219, removing\n",
      "Not enough responses at index 224\n",
      "Not enough responses at index 234\n",
      "Not enough responses at index 241\n",
      "Not enough responses at index 244\n",
      "Not enough responses at index 251\n",
      "Not enough responses at index 253\n",
      "Not enough responses at index 255\n",
      "Not enough responses at index 257\n",
      "Not enough responses at index 282\n",
      "Not enough responses at index 286\n",
      "Not enough responses at index 287\n",
      "Not enough responses at index 294\n",
      "Not enough responses at index 300\n",
      "Not enough responses at index 330\n",
      "Not enough responses at index 337\n",
      "Not enough responses at index 338\n",
      "Not enough responses at index 362\n",
      "Not enough responses at index 363\n",
      "Not enough responses at index 369\n",
      "Not enough responses at index 396\n",
      "Not enough responses at index 403\n",
      "Not enough responses at index 409\n",
      "Not enough responses at index 411\n",
      "Not enough responses at index 414\n",
      "Not enough responses at index 445\n",
      "Not enough responses at index 451\n",
      "Not enough responses at index 462\n",
      "Not enough responses at index 469\n",
      "Not enough responses at index 502\n",
      "Not enough responses at index 535\n",
      "Not enough responses at index 543\n",
      "Not enough responses at index 566\n",
      "Not enough responses at index 586\n",
      "Not enough responses at index 598\n",
      "Not enough responses at index 609\n",
      "Not enough responses at index 629\n",
      "(579/4188) 13.8% conversations valid\n",
      "Not enough responses at index 765\n",
      "Not enough responses at index 1140\n",
      "Not enough responses at index 1199\n",
      "(1221/6842) 17.8% conversations valid\n"
     ]
    }
   ],
   "source": [
    "# We want to consider conversations with a call-reply structure between two speakers, having at least five utterances\n",
    "def detect_interlocution(conv, min_utts, print=False):\n",
    "    '''\n",
    "    Finds whether the conversation has a call-reply structure between two speakers with at least min_utts utterances\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > conv - entire conversation object\n",
    "        > min_utts - the minimum number of utterances that constitute a valid conversation\n",
    "        > print - whether or not to print why the conversation was rejected\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > bool representing whether or not the conversation's longest path has the aforementioned structure\n",
    "    '''\n",
    "    # At the moment, only considering first longest path if there are multiple # TODO: Add functionality to examine all paths\n",
    "    try:\n",
    "        longest_path = conv.get_longest_paths()[0]\n",
    "    except ValueError as v:\n",
    "        if print:\n",
    "            print(v)\n",
    "            print('skipping...')\n",
    "        return False\n",
    "    \n",
    "    if len(longest_path) < min_utts:\n",
    "        if print:\n",
    "            print('Less than {} utterances in conversation\\nskipping...'.format(min_utts))\n",
    "        return False\n",
    "\n",
    "    speakers = []\n",
    "\n",
    "    for utt in longest_path:\n",
    "        speakers.append(utt.get_speaker().id)\n",
    "    \n",
    "    if len(set(speakers)) > 2:\n",
    "        if print:\n",
    "            print('More than 2 speakers in conversation\\nskipping...')\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def get_valid_conv_ids(corpus, exclude_last=True):\n",
    "    # Determine number of valid conversations\n",
    "    num_valid = 0\n",
    "    valid_conv_ids = []  # Will hold IDs of all valid converations\n",
    "    for conv in corpus.iter_conversations():\n",
    "        if detect_interlocution(conv, 5):\n",
    "            valid_conv_ids.append(conv.id)\n",
    "            num_valid += 1\n",
    "    \n",
    "    # Remove invalid\n",
    "    invalids = []\n",
    "    for j in range(len(valid_conv_ids)):\n",
    "        speakers, utts = get_speaker_utt_lists(corpus.get_conversation(valid_conv_ids[j]))\n",
    "        try:\n",
    "            pairs = get_pairs(speakers, utts)\n",
    "        except:\n",
    "            print('Error at index {}, removing'.format(j))\n",
    "            num_valid -= 1\n",
    "            invalids.append(j)\n",
    "            \n",
    "\n",
    "        # Remove the last utterance which contains the personal attack (or lack thereof)\n",
    "        if exclude_last:\n",
    "            last_utt = utts[-1]\n",
    "            for i in range(len(pairs['a_b'])):\n",
    "                if last_utt in pairs['a_b'][i]:\n",
    "                    del pairs['a_b'][i]\n",
    "                \n",
    "            for i in range(len(pairs['b_a'])):\n",
    "                if last_utt in pairs['b_a'][i]:\n",
    "                    del pairs['b_a'][i]\n",
    "\n",
    "        # Get markers from speaker a to b\n",
    "        # Note the order of a_b switched to b_a here. This is to be consistent with\n",
    "        # the notation of C(b,a) indicating the coordination of b to a\n",
    "        elicit_b_a = initialize_dict()\n",
    "        baseline_b_a = initialize_dict()\n",
    "        for a_b in pairs['a_b']:\n",
    "            u_a  = corpus.get_utterance(a_b[0])\n",
    "            u_b = corpus.get_utterance(a_b[1])\n",
    "            m_u_a = get_style_markers(u_a)\n",
    "            m_u_b = get_style_markers(u_b)\n",
    "            for k in m_u_a:\n",
    "                if m_u_a[k]:\n",
    "                    if m_u_a[k] == m_u_b[k]:  # If b responded to a with same style marker\n",
    "                        elicit_b_a[k] += 1\n",
    "                baseline_b_a[k] += m_u_b[k] # b's response contains m regardless of a's prompt\n",
    "        \n",
    "        # Get markers from speaker b to a\n",
    "        elicit_a_b = initialize_dict()\n",
    "        baseline_a_b = initialize_dict()\n",
    "        for b_a in pairs['b_a']:\n",
    "            u_b  = corpus.get_utterance(b_a[0])\n",
    "            u_a = corpus.get_utterance(b_a[1])\n",
    "            m_u_a = get_style_markers(u_a)\n",
    "            m_u_b = get_style_markers(u_b)\n",
    "            for k in m_u_b:  \n",
    "                if m_u_b[k]:\n",
    "                    if m_u_b[k] == m_u_a[k]:  # If a responded to b with same style marker\n",
    "                        elicit_a_b[k] += 1\n",
    "                baseline_a_b[k] += m_u_a[k] # If a's response contains m regardless of b's prompt\n",
    "        \n",
    "        \n",
    "        # Convert to probabilities, preserving raw baselines for LSM calculation\n",
    "        raw_b_a = baseline_b_a.copy()\n",
    "        raw_a_b = baseline_a_b.copy()\n",
    "        num_response_b_a = len(pairs['a_b'])  # Number of responses from b to a\n",
    "        num_response_a_b = len(pairs['b_a'])  # Number of responses from a to b\n",
    "        # Sometimes there aren't any responses from a to b or from b to a, continue if this is the case\n",
    "        if not num_response_a_b or not num_response_b_a:\n",
    "            print('Not enough responses at index {}'.format(j))\n",
    "            invalids.append(j)\n",
    "            num_valid -= 1\n",
    "\n",
    "    for i, inv in enumerate(invalids):\n",
    "        del valid_conv_ids[inv-i]\n",
    "\n",
    "    print('({}/{}) {:.1f}% conversations valid'.format(num_valid, len(corpus.get_conversation_ids()), num_valid*100/len(corpus.get_conversation_ids())))\n",
    "    return valid_conv_ids\n",
    "\n",
    "\n",
    "valid_conv_ids = get_valid_conv_ids(corpus)\n",
    "r_valid_conv_ids = get_valid_conv_ids(reddit_corpus)"
   ]
  },
  {
   "source": [
    "How to get a single conversation from list of valid IDs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('r_valid_conv_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(r_valid_conv_ids, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Conversation({'obj_type': 'conversation', 'meta': {'page_title': 'User talk:AngryParsley', 'page_id': 1282978, 'pair_id': '12941035.584.584', 'conversation_has_personal_attack': False, 'verified': True, 'pair_verified': True, 'annotation_year': '2018', 'split': 'train'}, 'vectors': [], 'tree': <convokit.model.utteranceNode.UtteranceNode object at 0x000001919C8E8910>, 'owner': <convokit.model.corpus.Corpus object at 0x000001919BF3E970>, 'id': '12451425.436.436'})"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "corpus.get_conversation(valid_conv_ids[4])"
   ]
  },
  {
   "source": [
    "Now that we have a list of valid conversations, let's find a way to isolate the utterances in a way that we can easily analyze for style accommodation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'valid_conv_ids' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-1bea911432cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpairs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mspeakers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_speaker_utt_lists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_conversation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_conv_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[0mget_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeakers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_conv_ids' is not defined"
     ]
    }
   ],
   "source": [
    "def get_speaker_utt_lists(conv):\n",
    "    '''\n",
    "    Generates lists of speaker IDs corresponding to utterances in conv, and gets utterances\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > conv - entire conversation object\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > speakers - list of speakers corresponding to each utterance\n",
    "        > utts - list of utterances from conversation\n",
    "    '''\n",
    "    # Recall, we use the first index longest path in our list of valid conversations\n",
    "    longest_path = conv.get_longest_paths()[0]\n",
    "\n",
    "    speakers = []\n",
    "    utts = []\n",
    "\n",
    "    for utt in longest_path:\n",
    "        utts.append(utt.id)\n",
    "        speakers.append(utt.get_speaker().id)\n",
    "\n",
    "    return speakers, utts\n",
    "\n",
    "def get_pairs(speakers, utts):\n",
    "    '''\n",
    "    Generates a dictionary of pairs of utterances, each pair representing a back and forth interaction\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > speakers - list of speakers corresponding to each utterance\n",
    "        > utts - list of utterances from conversation\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > pairs - dictionary with the following structure\n",
    "            a : ID of speaker a\n",
    "            b : ID of speaker b\n",
    "            a_b : [(tuple of 2 utterance IDs, first being from speaker a and second from speaker b), (...), ...]\n",
    "            b_a : [(tuple of 2 utterance IDs, first being from speaker b and second from speaker a), (...), ...]\n",
    "    '''\n",
    "    # TODO: Account for instances where a speaker speaks multiple times in a row. Combine those into a list of utteranes within the tuples\n",
    "    pairs = {\n",
    "        'a' : list(set(speakers))[0],\n",
    "        'b' : list(set(speakers))[1],\n",
    "        'a_b' : [],\n",
    "        'b_a' : []\n",
    "    }\n",
    "\n",
    "    # We'll say speaker a is the first speaker, and speaker b is the second.False\n",
    "    speaker_shift = [1 if speakers[i] != speakers[i-1] else 0 for i in range(1, len(speakers))]\n",
    "    speaker_shift.insert(0,0) # Prepend 0 (first utterance isn't a response)\n",
    "\n",
    "    for i in range(1, len(speakers)):\n",
    "       if speakers[i] == pairs['b'] and speakers[i-1] == pairs['a']:\n",
    "           pairs['a_b'].append((utts[i-1], utts[i]))\n",
    "       elif speakers[i] == pairs['a'] and speakers[i-1] == pairs['b']:\n",
    "           pairs['b_a'].append((utts[i-1], utts[i]))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "speakers, utts = get_speaker_utt_lists(corpus.get_conversation(valid_conv_ids[3]))\n",
    "get_pairs(speakers, utts)"
   ]
  },
  {
   "source": [
    "To measure style accommodation we have to measure the style markers in each utterance. That's what this following function is for.\n",
    "\n",
    "see https://spacy.io/models/en"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'conjunction, subordinating or preposition'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "spacy.explain('IN')"
   ]
  },
  {
   "source": [
    "## spaCy tags\n",
    "\n",
    "\n",
    "| spaCy tag \t| our tag \t| intended definition \t| actual                                   \t|\n",
    "|-----------\t|---------\t|---------------------\t|------------------------------------------\t|\n",
    "| PRP       \t| ppron   \t| personal pronoun    \t| personal pronoun                         \t|\n",
    "|           \t| ipron   \t| impersonal pronoun  \t|                                          \t|\n",
    "|           \t| article \t| article             \t|                                          \t|\n",
    "| CC        \t| conj    \t| conjunction         \t| coordinating conjunction                 \t|\n",
    "| IN        \t| prep    \t| preposition         \t| subordinating or preposition conjunction \t|\n",
    "| MD        \t| auxverb \t| auxiliary verb      \t| modal auxiliary verb                     \t|\n",
    "| RB        \t| adverb  \t| common adverb       \t| adverb                                   \t|\n",
    "|           \t| negate  \t| negation            \t|                                          \t|\n",
    "|           \t| quant   \t| quantifier          \t|                                          \t|\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'valid_conv_ids' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-04a421d94b68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mspeakers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_speaker_utt_lists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_conversation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_conv_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeakers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[0mget_style_markers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_utterance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_conv_ids' is not defined"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "def get_style_markers(utt):\n",
    "    '''\n",
    "    Returns a dictionary containing the number of style markers in an utterance\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > utt - a single utterance\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > m - dictionary with the following key value pairs\n",
    "            ppron : # personal pronouns\n",
    "            ipron : # impersonal pronouns\n",
    "            article : # articles\n",
    "            conj : # conjunctions\n",
    "            prep : # prepositions\n",
    "            auxverb : # auxiliary verbs\n",
    "            adverb : # common adverbs\n",
    "            negate : # negations\n",
    "            quant : # quantifiers\n",
    "    '''\n",
    "    m = {\n",
    "        'ppron' : 0,\n",
    "        'ipron' : 0,\n",
    "        'article' : 0,\n",
    "        'conj' : 0,\n",
    "        'prep' : 0,\n",
    "        'auxverb' : 0,\n",
    "        'adverb' : 0,\n",
    "        'negate' : 0,\n",
    "        'quant' : 0\n",
    "    }\n",
    "\n",
    "    # Tokenize text\n",
    "    text = utt.text.lower().split()\n",
    "\n",
    "    # Analyze\n",
    "    for word in text:\n",
    "        if word in liwc_dic.keys():\n",
    "            m[liwc_dic[word]] += 1\n",
    "    \n",
    "    # Convert to boolean\n",
    "    for k in m.keys():\n",
    "        if m[k]:\n",
    "            m[k] = 1\n",
    "\n",
    "    return m\n",
    "\n",
    "speakers, utts = get_speaker_utt_lists(corpus.get_conversation(valid_conv_ids[3]))\n",
    "print(set(speakers))\n",
    "get_style_markers(corpus.get_utterance(utts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'valid_conv_ids' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-32237d9d0622>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;31m#     C = wiki_measure_coordination(conv, corpus = corpus, exclude_last = True, print_output=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_conversation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_conv_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwiki_measure_coordination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude_last\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_conv_ids' is not defined"
     ]
    }
   ],
   "source": [
    "def initialize_dict():\n",
    "    return {\n",
    "        'ppron' : 0,\n",
    "        'ipron' : 0,\n",
    "        'article' : 0,\n",
    "        'conj' : 0,\n",
    "        'prep' : 0,\n",
    "        'auxverb' : 0,\n",
    "        'adverb' : 0,\n",
    "        'negate' : 0,\n",
    "        'quant' : 0\n",
    "    }\n",
    "\n",
    "def wiki_measure_coordination(conv, corpus, exclude_last, print_output=False):\n",
    "    '''\n",
    "    Assumes the converation will only have two speakers\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > conv - entire conversation object\n",
    "        > print_output - whether to print medial variables\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > C - dictionary with following key value pairs\n",
    "            convID : ID of conversation\n",
    "            a : ID of speaker a\n",
    "            b : ID of speaker b\n",
    "            num_response_b_a : number of responses from b to a\n",
    "            num_response_a_b : number of responses from a to b\n",
    "            C_b_a : dictrionary of asymmetric accomodation from speaker b to speaker a\n",
    "            C_a_b : dictionary of asymmetric accomodation from speaker a to speaker b\n",
    "            LSM : dictionary of symmetric accomodation between both speakers\n",
    "            mean_C_b_a : average accomodation from b towards a across valid markers\n",
    "            mean_C_a_b : average accomodation from a towards b across valid markers\n",
    "            mean_LSM : average of symmetric accommodation\n",
    "            valid_markers : list of valid markers\n",
    "    '''\n",
    "    # ~~~~~~~~~~~ VARIABLES ~~~~~~~~~~~\n",
    "    #     > pairs - dictionary containing interlocution information\n",
    "    #     > raw_b_a - number of style markers used in all responses from b to a\n",
    "    #     > raw_a_b - number of style markers used in all responses from a to b\n",
    "    #     > baseline_b_a - probability of style markers in b's response to a\n",
    "    #     > baseline_a_b - probability of style markers in a's response to b\n",
    "    #     > elicit_b_a - probability of style markers in b's response to a given a exhibited the same marker\n",
    "    #     > elicit_a_b - probability of style markers in a's response to a given b exhibited the same marker\n",
    "    \n",
    "    speakers, utts = get_speaker_utt_lists(conv)\n",
    "    try:\n",
    "        pairs = get_pairs(speakers, utts)\n",
    "    except IndexError as err:\n",
    "        print(speakers)\n",
    "        print(utts)\n",
    "        print(err)\n",
    "        print('Error!')\n",
    "        return None\n",
    "    personal_attack = corpus.get_utterance(utts[-1]).retrieve_meta('comment_has_personal_attack')\n",
    "\n",
    "    # Remove the last utterance which contains the personal attack (or lack thereof)\n",
    "    if exclude_last:\n",
    "        last_utt = utts[-1]\n",
    "        for i in range(len(pairs['a_b'])):\n",
    "            if last_utt in pairs['a_b'][i]:\n",
    "                del pairs['a_b'][i]\n",
    "            \n",
    "        for i in range(len(pairs['b_a'])):\n",
    "            if last_utt in pairs['b_a'][i]:\n",
    "                del pairs['b_a'][i]\n",
    "\n",
    "    # Get markers from speaker a to b\n",
    "    # Note the order of a_b switched to b_a here. This is to be consistent with\n",
    "    # the notation of C(b,a) indicating the coordination of b to a\n",
    "    elicit_b_a = initialize_dict()\n",
    "    baseline_b_a = initialize_dict()\n",
    "    for a_b in pairs['a_b']:\n",
    "        u_a  = corpus.get_utterance(a_b[0])\n",
    "        u_b = corpus.get_utterance(a_b[1])\n",
    "        m_u_a = get_style_markers(u_a)\n",
    "        m_u_b = get_style_markers(u_b)\n",
    "        for k in m_u_a:\n",
    "            if m_u_a[k]:\n",
    "                if m_u_a[k] == m_u_b[k]:  # If b responded to a with same style marker\n",
    "                    elicit_b_a[k] += 1\n",
    "            baseline_b_a[k] += m_u_b[k] # b's response contains m regardless of a's prompt\n",
    "    \n",
    "    # Get markers from speaker b to a\n",
    "    elicit_a_b = initialize_dict()\n",
    "    baseline_a_b = initialize_dict()\n",
    "    for b_a in pairs['b_a']:\n",
    "        u_b  = corpus.get_utterance(b_a[0])\n",
    "        u_a = corpus.get_utterance(b_a[1])\n",
    "        m_u_a = get_style_markers(u_a)\n",
    "        m_u_b = get_style_markers(u_b)\n",
    "        for k in m_u_b:  \n",
    "            if m_u_b[k]:\n",
    "                if m_u_b[k] == m_u_a[k]:  # If a responded to b with same style marker\n",
    "                    elicit_a_b[k] += 1\n",
    "            baseline_a_b[k] += m_u_a[k] # If a's response contains m regardless of b's prompt\n",
    "    \n",
    "    \n",
    "    # Convert to probabilities, preserving raw baselines for LSM calculation\n",
    "    raw_b_a = baseline_b_a.copy()\n",
    "    raw_a_b = baseline_a_b.copy()\n",
    "    num_response_b_a = len(pairs['a_b'])  # Number of responses from b to a\n",
    "    num_response_a_b = len(pairs['b_a'])  # Number of responses from a to b\n",
    "    # Sometimes there aren't any responses from a to b or from b to a, continue if this is the case\n",
    "    if not num_response_a_b or not num_response_b_a:\n",
    "        print('Only one speaker in conversation, skipping')\n",
    "        return None\n",
    "    for k in elicit_a_b.keys():  # Could be any dictionary, they all have the same keys\n",
    "        elicit_b_a[k] = elicit_b_a[k] / num_response_b_a \n",
    "        baseline_b_a[k] = baseline_b_a[k] / num_response_b_a\n",
    "        elicit_a_b[k] = elicit_a_b[k] / num_response_a_b\n",
    "        baseline_a_b[k] = baseline_a_b[k] / num_response_a_b\n",
    "\n",
    "    # Determine asymmetric and symmetric accomodation\n",
    "    C_b_a = initialize_dict() # Accomodation of b towards a\n",
    "    C_a_b = initialize_dict() # Accomodation of a towards b\n",
    "    LSM = initialize_dict()\n",
    "    for k in C_b_a.keys():\n",
    "        if baseline_b_a[k] and baseline_a_b[k]:  # If a and b both exhibited marker m at some point\n",
    "            C_b_a[k] = baseline_b_a[k] - elicit_b_a[k]\n",
    "            C_a_b[k] = baseline_a_b[k] - elicit_a_b[k]\n",
    "            LSM[k] = 1 - abs(raw_a_b[k] - raw_b_a[k]) / (raw_a_b[k] + raw_b_a[k] + 0.0001)\n",
    "        else:                                    # Else, the metric is undefined for marker m\n",
    "            C_b_a[k] = None  # Set to None if there is no data\n",
    "            C_a_b[k] = None\n",
    "            LSM[k] = None\n",
    "\n",
    "    # Get averages across asymmetric measure\n",
    "    valid_markers = []\n",
    "    mean_C_b_a = 0\n",
    "    mean_C_a_b = 0\n",
    "    mean_LSM = 0\n",
    "    for k in C_b_a.keys():\n",
    "        if C_b_a[k] is not None:\n",
    "            mean_C_b_a += C_b_a[k]\n",
    "            mean_C_a_b += C_a_b[k]\n",
    "            mean_LSM += LSM[k]\n",
    "            valid_markers.append(k)\n",
    "    if valid_markers:\n",
    "        mean_C_b_a /= len(valid_markers)\n",
    "        mean_C_a_b /= len(valid_markers)\n",
    "        mean_LSM /= len(valid_markers)\n",
    "\n",
    "    # Construct dictionary to return\n",
    "    C = {\n",
    "        'convID' : conv.id,\n",
    "        'a' : pairs['a'],\n",
    "        'b' : pairs['b'],\n",
    "        'num_response_b_a' : len(pairs['b_a']),\n",
    "        'num_response_a_b' : len(pairs['a_b']),\n",
    "        'C_b_a' : C_b_a,\n",
    "        'C_a_b' : C_a_b,\n",
    "        'LSM' : LSM,\n",
    "        'mean_C_b_a' : mean_C_b_a,\n",
    "        'mean_C_a_b' : mean_C_a_b,\n",
    "        'mean_LSM' : mean_LSM,\n",
    "        'valid_markers' : valid_markers,\n",
    "        'corpus' : 'wikipedia',\n",
    "        'personal_attack' : personal_attack\n",
    "    } \n",
    "\n",
    "    if print_output:\n",
    "        print('pairs: ', pairs)\n",
    "        print('\\nraw_b_a: ', raw_b_a)\n",
    "        print('raw_a_b: ', raw_a_b)\n",
    "        print('\\nelicit_b_a: ', elicit_b_a)\n",
    "        print('elicit_a_b: ', elicit_a_b)\n",
    "        print('\\nbaseline_b_a: ', baseline_b_a)\n",
    "        print('baseline_a_b: ', baseline_a_b)\n",
    "        print('\\nC_b_a: ', C_b_a)\n",
    "        print('C_a_b: ', C_a_b)\n",
    "        print('\\nLSM: ', LSM)\n",
    "        print('\\nmean_C_b_a: ', mean_C_b_a)\n",
    "        print('mean_C_a_b: ', mean_C_a_b)\n",
    "        \n",
    "    return C\n",
    "\n",
    "# for i in range(len(valid_conv_ids)):\n",
    "#     conv = corpus.get_conversation(valid_conv_ids[i])\n",
    "#     C = wiki_measure_coordination(conv, corpus = corpus, exclude_last = True, print_output=False)\n",
    "\n",
    "conv = corpus.get_conversation(valid_conv_ids[10])\n",
    "C = wiki_measure_coordination(conv, corpus = corpus, exclude_last=True, print_output=False)\n",
    "\n",
    "print_coordination(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'r_valid_conv_ids' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-56c0886c1f5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_valid_conv_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m     \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreddit_corpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_conversation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_valid_conv_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreddit_measure_coordination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreddit_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'r_valid_conv_ids' is not defined"
     ]
    }
   ],
   "source": [
    "def reddit_measure_coordination(conv, corpus, print_output=False):\n",
    "    '''\n",
    "    Assumes the converation will only have two speakers\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > conv - entire conversation object\n",
    "        > print_output - whether to print medial variables\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > C - dictionary with following key value pairs\n",
    "            convID : ID of conversation\n",
    "            a : ID of speaker a\n",
    "            b : ID of speaker b\n",
    "            num_response_b_a : number of responses from b to a\n",
    "            num_response_a_b : number of responses from a to b\n",
    "            C_b_a : dictrionary of asymmetric accomodation from speaker b to speaker a\n",
    "            C_a_b : dictionary of asymmetric accomodation from speaker a to speaker b\n",
    "            LSM : dictionary of symmetric accomodation between both speakers\n",
    "            mean_C_b_a : average accomodation from b towards a across valid markers\n",
    "            mean_C_a_b : average accomodation from a towards b across valid markers\n",
    "            mean_LSM : average of symmetric accommodation\n",
    "            valid_markers : list of valid markers\n",
    "    '''\n",
    "    # ~~~~~~~~~~~ VARIABLES ~~~~~~~~~~~\n",
    "    #     > pairs - dictionary containing interlocution information\n",
    "    #     > raw_b_a - number of style markers used in all responses from b to a\n",
    "    #     > raw_a_b - number of style markers used in all responses from a to b\n",
    "    #     > baseline_b_a - probability of style markers in b's response to a\n",
    "    #     > baseline_a_b - probability of style markers in a's response to b\n",
    "    #     > elicit_b_a - probability of style markers in b's response to a given a exhibited the same marker\n",
    "    #     > elicit_a_b - probability of style markers in a's response to a given b exhibited the same marker\n",
    "    \n",
    "    speakers, utts = get_speaker_utt_lists(conv)\n",
    "    try:\n",
    "        pairs = get_pairs(speakers, utts)\n",
    "    except IndexError as err:\n",
    "        print(speakers)\n",
    "        print(utts)\n",
    "        print(err)\n",
    "        print('Error!')\n",
    "        return None\n",
    "\n",
    "    # Get markers from speaker a to b\n",
    "    # Note the order of a_b switched to b_a here. This is to be consistent with\n",
    "    # the notation of C(b,a) indicating the coordination of b to a\n",
    "    elicit_b_a = initialize_dict()\n",
    "    baseline_b_a = initialize_dict()\n",
    "    for a_b in pairs['a_b']:\n",
    "        u_a  = corpus.get_utterance(a_b[0])\n",
    "        u_b = corpus.get_utterance(a_b[1])\n",
    "        m_u_a = get_style_markers(u_a)\n",
    "        m_u_b = get_style_markers(u_b)\n",
    "        for k in m_u_a:\n",
    "            if m_u_a[k]:\n",
    "                if m_u_a[k] == m_u_b[k]:  # If b responded to a with same style marker\n",
    "                    elicit_b_a[k] += 1\n",
    "            baseline_b_a[k] += m_u_b[k] # b's response contains m regardless of a's prompt\n",
    "    \n",
    "    # Get markers from speaker b to a\n",
    "    elicit_a_b = initialize_dict()\n",
    "    baseline_a_b = initialize_dict()\n",
    "    for b_a in pairs['b_a']:\n",
    "        u_b  = corpus.get_utterance(b_a[0])\n",
    "        u_a = corpus.get_utterance(b_a[1])\n",
    "        m_u_a = get_style_markers(u_a)\n",
    "        m_u_b = get_style_markers(u_b)\n",
    "        for k in m_u_b:  \n",
    "            if m_u_b[k]:\n",
    "                if m_u_b[k] == m_u_a[k]:  # If a responded to b with same style marker\n",
    "                    elicit_a_b[k] += 1\n",
    "            baseline_a_b[k] += m_u_a[k] # If a's response contains m regardless of b's prompt\n",
    "    \n",
    "    \n",
    "    # Convert to probabilities, preserving raw baselines for LSM calculation\n",
    "    raw_b_a = baseline_b_a.copy()\n",
    "    raw_a_b = baseline_a_b.copy()\n",
    "    num_response_b_a = len(pairs['a_b'])  # Number of responses from b to a\n",
    "    num_response_a_b = len(pairs['b_a'])  # Number of responses from a to b\n",
    "    # Sometimes there aren't any responses from a to b or from b to a, continue if this is the case\n",
    "    if not num_response_a_b or not num_response_b_a:\n",
    "        print('Only one speaker in conversation, skipping')\n",
    "        return None\n",
    "    for k in elicit_a_b.keys():  # Could be any dictionary, they all have the same keys\n",
    "        elicit_b_a[k] = elicit_b_a[k] / num_response_b_a \n",
    "        baseline_b_a[k] = baseline_b_a[k] / num_response_b_a\n",
    "        elicit_a_b[k] = elicit_a_b[k] / num_response_a_b\n",
    "        baseline_a_b[k] = baseline_a_b[k] / num_response_a_b\n",
    "\n",
    "    # Determine asymmetric and symmetric accomodation\n",
    "    C_b_a = initialize_dict() # Accomodation of b towards a\n",
    "    C_a_b = initialize_dict() # Accomodation of a towards b\n",
    "    LSM = initialize_dict()\n",
    "    for k in C_b_a.keys():\n",
    "        if baseline_b_a[k] and baseline_a_b[k]:  # If a and b both exhibited marker m at some point\n",
    "            C_b_a[k] = baseline_b_a[k] - elicit_b_a[k]\n",
    "            C_a_b[k] = baseline_a_b[k] - elicit_a_b[k]\n",
    "            LSM[k] = 1 - abs(raw_a_b[k] - raw_b_a[k]) / (raw_a_b[k] + raw_b_a[k] + 0.0001)\n",
    "        else:                                    # Else, the metric is undefined for marker m\n",
    "            C_b_a[k] = None  # Set to None if there is no data\n",
    "            C_a_b[k] = None\n",
    "            LSM[k] = None\n",
    "\n",
    "    # Get averages across asymmetric measure\n",
    "    valid_markers = []\n",
    "    mean_C_b_a = 0\n",
    "    mean_C_a_b = 0\n",
    "    mean_LSM = 0\n",
    "    for k in C_b_a.keys():\n",
    "        if C_b_a[k] is not None:\n",
    "            mean_C_b_a += C_b_a[k]\n",
    "            mean_C_a_b += C_a_b[k]\n",
    "            mean_LSM += LSM[k]\n",
    "            valid_markers.append(k)\n",
    "    if valid_markers:\n",
    "        mean_C_b_a /= len(valid_markers)\n",
    "        mean_C_a_b /= len(valid_markers)\n",
    "        mean_LSM /= len(valid_markers)\n",
    "\n",
    "    # Construct dictionary to return\n",
    "    C = {\n",
    "        'convID' : conv.id,\n",
    "        'a' : pairs['a'],\n",
    "        'b' : pairs['b'],\n",
    "        'num_response_b_a' : len(pairs['b_a']),\n",
    "        'num_response_a_b' : len(pairs['a_b']),\n",
    "        'C_b_a' : C_b_a,\n",
    "        'C_a_b' : C_a_b,\n",
    "        'LSM' : LSM,\n",
    "        'mean_C_b_a' : mean_C_b_a,\n",
    "        'mean_C_a_b' : mean_C_a_b,\n",
    "        'mean_LSM' : mean_LSM,\n",
    "        'valid_markers' : valid_markers,\n",
    "        'corpus' : 'reddit',\n",
    "        'personal_attack' : conv.meta['has_removed_comment']\n",
    "    } \n",
    "    \n",
    "    if print_output:\n",
    "        print('pairs: ', pairs)\n",
    "        print('\\nraw_b_a: ', raw_b_a)\n",
    "        print('raw_a_b: ', raw_a_b)\n",
    "        print('\\nelicit_b_a: ', elicit_b_a)\n",
    "        print('elicit_a_b: ', elicit_a_b)\n",
    "        print('\\nbaseline_b_a: ', baseline_b_a)\n",
    "        print('baseline_a_b: ', baseline_a_b)\n",
    "        print('\\nC_b_a: ', C_b_a)\n",
    "        print('C_a_b: ', C_a_b)\n",
    "        print('\\nLSM: ', LSM)\n",
    "        print('\\nmean_C_b_a: ', mean_C_b_a)\n",
    "        print('mean_C_a_b: ', mean_C_a_b)\n",
    "        \n",
    "    return C\n",
    "\n",
    "for i in range(len(r_valid_conv_ids)):\n",
    "    conv = reddit_corpus.get_conversation(r_valid_conv_ids[i])\n",
    "    C = reddit_measure_coordination(conv, corpus = reddit_corpus, print_output=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'C' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-d3cc4e3f19c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{} : {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mprint_coordination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'C' is not defined"
     ]
    }
   ],
   "source": [
    "def print_coordination(C):\n",
    "    '''\n",
    "    Prints a coordination dictionary (output from measure_coordination) legibly\n",
    "    '''\n",
    "    for k in C.keys():\n",
    "        if isinstance(C[k], dict):\n",
    "            print('\\n~~ {} ~~'.format(k))\n",
    "            for m in C[k].keys():\n",
    "                if C[k][m] is not None:\n",
    "                    print('     {} : {:.2f}'.format(m, C[k][m]))\n",
    "                else:\n",
    "                    print('     {} : None'.format(m))\n",
    "            if k == 'LSM':\n",
    "                print('\\n')\n",
    "        else:\n",
    "            print('{} : {}'.format(k, C[k]))\n",
    "\n",
    "print_coordination(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}