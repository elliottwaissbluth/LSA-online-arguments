{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1131efc7635b497546d7e8fbc76ad9d1f9d5d5d7857bcde935d6feea39d08984"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Conversations gone awry, Wikipedia version"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset already exists at C:\\Users\\ewais\\.convokit\\downloads\\conversations-gone-awry-corpus\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, download\n",
    "import spacy\n",
    "import pickle\n",
    "corpus = Corpus(filename=download('conversations-gone-awry-corpus'))\n",
    "\n",
    "# Load liwc_dic\n",
    "with open('liwc_dic.pkl', 'rb') as handle:\n",
    "    liwc_dic = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Speakers: 8069\nNumber of Utterances: 30021\nNumber of Conversations: 4188\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gareth Griffith-Jones\n"
     ]
    }
   ],
   "source": [
    "utt = corpus.random_utterance()\n",
    "print(utt.speaker.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Conversation('id': '115486269.9877.9877', 'utterances': ['115486269.9877.9877', '115486269.9927.9877', '115487861.10037.10037', '115490705.10368.10368', '115492642.10886.10886', '115738559.11855.11855', '116133932.12559.12559'], 'meta': {'page_title': 'Talk:Slam dunk', 'page_id': 1221480, 'pair_id': '37617534.406.406', 'conversation_has_personal_attack': True, 'verified': False, 'pair_verified': False, 'annotation_year': '2019', 'split': 'train'})\n"
     ]
    }
   ],
   "source": [
    "convo = corpus.random_conversation()\n",
    "print(convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['115486269.9877.9877', '115487861.10037.10037', '115490705.10368.10368']\n['Feeeshboy', 'Natster237', 'Feeeshboy']\n['115486269.9877.9877', '115487861.10037.10037', '115492642.10886.10886']\n['Feeeshboy', 'Natster237', 'Natster237']\n['115486269.9877.9877', '115738559.11855.11855', '116133932.12559.12559']\n['Feeeshboy', 'Justbc', 'Feeeshboy']\n"
     ]
    }
   ],
   "source": [
    "paths = convo.get_longest_paths()\n",
    "for path in paths:\n",
    "    print([utt.id for utt in path])\n",
    "    print([utt.get_speaker().id for utt in path])"
   ]
  },
  {
   "source": [
    "## Utterance features\n",
    "- **id**: index of the utterance\n",
    "- **conversation_id**: id of the first utterance in the converstaion this utterance belongs to\n",
    "- **reply-to**: index of the utterance to which this utterance replies to (None if not a reply)\n",
    "- **speaker**: the speaker who authored the utterance\n",
    "- **timestamp**: timestamp of utterance\n",
    "- **text**: textual content of the utterance\n",
    "- **meta**: metadata for each utterance\n",
    "    - **is_section_header**: whether the utterance is a conversation \"title\" or \"subject\" (if true, the utterance should be ignored)\n",
    "    - **comment_has_personal_attack**: whether this comment was judged by 3 crowdsourced annotators to contain a personal comment_has_personal_attack\n",
    "    - **parsed**: SpaCy parsed version of the utterance text\n",
    "        - **rt**: ??\n",
    "        - **toks**: List of parsed tokens\n",
    "            - **tok**: the token (word, punctuation, etc.)\n",
    "            - **tag**: Detailed part of speech tag\n",
    "            - **dep**: syntactic dependency, i.e. the relation between the tokens\n",
    "            - **up**: list related to dn, not sure how\n",
    "            - **dn**: list related to up, not sure how"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Conversation features\n",
    "\n",
    "- **id**: id of the conversation\n",
    "- **utterances**: ids of utterances in the conversation (in order I presume)\n",
    "- **meta**: conversation metadata\n",
    "    - **page_title**: title of page under which conversation is occurring\n",
    "    - **page_id**: unique numerical id of the talk page\n",
    "    - **pair_id**: the id of the conversation that this comment's conversation is paired with\n",
    "    - **conversation_has_personal_attack**: whether any comment in this comment's conversation contains a personal attack\n",
    "    - **verified**: whether the personal attack label has been verified by an internal annotator\n",
    "    - **pair_verified**: whether the personal attack label has been double checked by the internal annotator\n",
    "    - **annotation_year**: self explanatory\n",
    "    - **split**: (train, test, or val) whether this conversation was used as train, test, or val in \"Trouble on the Horizon\"\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First, we want to find the conversations that are easy to analyze, i.e. have a structure like (a -> b -> a -> b -> ...). detect_interlocution should reveal those conversations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(635/4188) 15.2% conversations valid\n"
     ]
    }
   ],
   "source": [
    "# We want to consider conversations with a call-reply structure between two speakers, having at least five utterances\n",
    "def detect_interlocution(conv, min_utts, print=False):\n",
    "    '''\n",
    "    Finds whether the conversation has a call-reply structure between two speakers with at least min_utts utterances\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > conv - entire conversation object\n",
    "        > min_utts - the minimum number of utterances that constitute a valid conversation\n",
    "        > print - whether or not to print why the conversation was rejected\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > bool representing whether or not the conversation's longest path has the aforementioned structure\n",
    "    '''\n",
    "    # At the moment, only considering first longest path if there are multiple # TODO: Add functionality to examine all paths\n",
    "    try:\n",
    "        longest_path = conv.get_longest_paths()[0]\n",
    "    except ValueError as v:\n",
    "        if print:\n",
    "            print(v)\n",
    "            print('skipping...')\n",
    "        return False\n",
    "    \n",
    "    if len(longest_path) < min_utts:\n",
    "        if print:\n",
    "            print('Less than {} utterances in conversation\\nskipping...'.format(min_utts))\n",
    "        return False\n",
    "\n",
    "    speakers = []\n",
    "\n",
    "    for utt in longest_path:\n",
    "        speakers.append(utt.get_speaker().id)\n",
    "    \n",
    "    if len(set(speakers)) > 2:\n",
    "        if print:\n",
    "            print('More than 2 speakers in conversation\\nskipping...')\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Determine number of valid conversations\n",
    "num_valid = 0\n",
    "valid_conv_ids = []  # Will hold IDs of all valid converations\n",
    "for conv in corpus.iter_conversations():\n",
    "    if detect_interlocution(conv, 5):\n",
    "        valid_conv_ids.append(conv.id)\n",
    "        num_valid += 1\n",
    "\n",
    "print('({}/{}) {:.1f}% conversations valid'.format(num_valid, len(corpus.get_conversation_ids()), num_valid*100/len(corpus.get_conversation_ids())))\n"
   ]
  },
  {
   "source": [
    "How to get a single conversation from list of valid IDs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Conversation({'obj_type': 'conversation', 'meta': {'page_title': 'User talk:AngryParsley', 'page_id': 1282978, 'pair_id': '12941035.584.584', 'conversation_has_personal_attack': False, 'verified': True, 'pair_verified': True, 'annotation_year': '2018', 'split': 'train'}, 'vectors': [], 'tree': <convokit.model.utteranceNode.UtteranceNode object at 0x000001919C8E8910>, 'owner': <convokit.model.corpus.Corpus object at 0x000001919BF3E970>, 'id': '12451425.436.436'})"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "corpus.get_conversation(valid_conv_ids[4])"
   ]
  },
  {
   "source": [
    "Now that we have a list of valid conversations, let's find a way to isolate the utterances in a way that we can easily analyze for style accommodation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'a': 'Mike Garcia',\n",
       " 'b': '66.36.136.123',\n",
       " 'a_b': [('15832939.3151.3151', '15833000.3256.3256')],\n",
       " 'b_a': [('15832773.3035.3019', '15832939.3151.3151'),\n",
       "  ('15833000.3256.3256', '15833036.3275.3275')]}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "def get_speaker_utt_lists(conv):\n",
    "    '''\n",
    "    Generates lists of speaker IDs corresponding to utterances in conv, and gets utterances\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > conv - entire conversation object\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > speakers - list of speakers corresponding to each utterance\n",
    "        > utts - list of utterances from conversation\n",
    "    '''\n",
    "    # Recall, we use the first index longest path in our list of valid conversations\n",
    "    longest_path = conv.get_longest_paths()[0]\n",
    "\n",
    "    speakers = []\n",
    "    utts = []\n",
    "\n",
    "    for utt in longest_path:\n",
    "        utts.append(utt.id)\n",
    "        speakers.append(utt.get_speaker().id)\n",
    "\n",
    "    return speakers, utts\n",
    "\n",
    "def get_pairs(speakers, utts):\n",
    "    '''\n",
    "    Generates a dictionary of pairs of utterances, each pair representing a back and forth interaction\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > speakers - list of speakers corresponding to each utterance\n",
    "        > utts - list of utterances from conversation\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > pairs - dictionary with the following structure\n",
    "            a : ID of speaker a\n",
    "            b : ID of speaker b\n",
    "            a_b : [(tuple of 2 utterance IDs, first being from speaker a and second from speaker b), (...), ...]\n",
    "            b_a : [(tuple of 2 utterance IDs, first being from speaker b and second from speaker a), (...), ...]\n",
    "    '''\n",
    "    # TODO: Account for instances where a speaker speaks multiple times in a row. Combine those into a list of utteranes within the tuples\n",
    "    pairs = {\n",
    "        'a' : list(set(speakers))[0],\n",
    "        'b' : list(set(speakers))[1],\n",
    "        'a_b' : [],\n",
    "        'b_a' : []\n",
    "    }\n",
    "\n",
    "    # We'll say speaker a is the first speaker, and speaker b is the second.False\n",
    "    speaker_shift = [1 if speakers[i] != speakers[i-1] else 0 for i in range(1, len(speakers))]\n",
    "    speaker_shift.insert(0,0) # Prepend 0 (first utterance isn't a response)\n",
    "\n",
    "    for i in range(1, len(speakers)):\n",
    "       if speakers[i] == pairs['b'] and speakers[i-1] == pairs['a']:\n",
    "           pairs['a_b'].append((utts[i-1], utts[i]))\n",
    "       elif speakers[i] == pairs['a'] and speakers[i-1] == pairs['b']:\n",
    "           pairs['b_a'].append((utts[i-1], utts[i]))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "speakers, utts = get_speaker_utt_lists(corpus.get_conversation(valid_conv_ids[3]))\n",
    "get_pairs(speakers, utts)"
   ]
  },
  {
   "source": [
    "To measure style accommodation we have to measure the style markers in each utterance. That's what this following function is for.\n",
    "\n",
    "see https://spacy.io/models/en"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'conjunction, subordinating or preposition'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "spacy.explain('IN')"
   ]
  },
  {
   "source": [
    "## spaCy tags\n",
    "\n",
    "\n",
    "| spaCy tag \t| our tag \t| intended definition \t| actual                                   \t|\n",
    "|-----------\t|---------\t|---------------------\t|------------------------------------------\t|\n",
    "| PRP       \t| ppron   \t| personal pronoun    \t| personal pronoun                         \t|\n",
    "|           \t| ipron   \t| impersonal pronoun  \t|                                          \t|\n",
    "|           \t| article \t| article             \t|                                          \t|\n",
    "| CC        \t| conj    \t| conjunction         \t| coordinating conjunction                 \t|\n",
    "| IN        \t| prep    \t| preposition         \t| subordinating or preposition conjunction \t|\n",
    "| MD        \t| auxverb \t| auxiliary verb      \t| modal auxiliary verb                     \t|\n",
    "| RB        \t| adverb  \t| common adverb       \t| adverb                                   \t|\n",
    "|           \t| negate  \t| negation            \t|                                          \t|\n",
    "|           \t| quant   \t| quantifier          \t|                                          \t|\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Mike Garcia', '66.36.136.123'}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ppron': 1,\n",
       " 'ipron': 1,\n",
       " 'article': 1,\n",
       " 'conj': 1,\n",
       " 'prep': 1,\n",
       " 'auxverb': 1,\n",
       " 'adverb': 1,\n",
       " 'negate': 0,\n",
       " 'quant': 0}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "def get_style_markers(utt):\n",
    "    '''\n",
    "    Returns a dictionary containing the number of style markers in an utterance\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > utt - a single utterance\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > m - dictionary with the following key value pairs\n",
    "            ppron : # personal pronouns\n",
    "            ipron : # impersonal pronouns\n",
    "            article : # articles\n",
    "            conj : # conjunctions\n",
    "            prep : # prepositions\n",
    "            auxverb : # auxiliary verbs\n",
    "            adverb : # common adverbs\n",
    "            negate : # negations\n",
    "            quant : # quantifiers\n",
    "    '''\n",
    "    m = {\n",
    "        'ppron' : 0,\n",
    "        'ipron' : 0,\n",
    "        'article' : 0,\n",
    "        'conj' : 0,\n",
    "        'prep' : 0,\n",
    "        'auxverb' : 0,\n",
    "        'adverb' : 0,\n",
    "        'negate' : 0,\n",
    "        'quant' : 0\n",
    "    }\n",
    "\n",
    "    # Tokenize text\n",
    "    text = utt.text.lower().split()\n",
    "\n",
    "    # Analyze\n",
    "    for word in text:\n",
    "        if word in liwc_dic.keys():\n",
    "            m[liwc_dic[word]] += 1\n",
    "    \n",
    "    # Convert to boolean\n",
    "    for k in m.keys():\n",
    "        if m[k]:\n",
    "            m[k] = 1\n",
    "\n",
    "    return m\n",
    "\n",
    "speakers, utts = get_speaker_utt_lists(corpus.get_conversation(valid_conv_ids[3]))\n",
    "print(set(speakers))\n",
    "get_style_markers(corpus.get_utterance(utts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pairs:  {'a': 'The undertow', 'b': 'Indubitably', 'a_b': [('162766001.2432.2409', '162770113.2620.2620'), ('162775518.2849.2849', '162776180.2956.2956'), ('162791351.3026.3026', '162791625.3109.3109')], 'b_a': [('162770113.2620.2620', '162775518.2849.2849'), ('162776180.2956.2956', '162791351.3026.3026')]}\n\nraw_b_a:  {'ppron': 1, 'ipron': 2, 'article': 3, 'conj': 2, 'prep': 2, 'auxverb': 2, 'adverb': 3, 'negate': 1, 'quant': 0}\nraw_a_b:  {'ppron': 2, 'ipron': 2, 'article': 2, 'conj': 2, 'prep': 1, 'auxverb': 2, 'adverb': 2, 'negate': 2, 'quant': 0}\n\nelicit_b_a:  {'ppron': 0.3333333333333333, 'ipron': 0.6666666666666666, 'article': 1.0, 'conj': 0.6666666666666666, 'prep': 0.6666666666666666, 'auxverb': 0.6666666666666666, 'adverb': 1.0, 'negate': 0.3333333333333333, 'quant': 0.0}\nelicit_a_b:  {'ppron': 0.5, 'ipron': 1.0, 'article': 1.0, 'conj': 0.5, 'prep': 0.5, 'auxverb': 1.0, 'adverb': 1.0, 'negate': 0.5, 'quant': 0.0}\n\nbaseline_b_a:  {'ppron': 0.3333333333333333, 'ipron': 0.6666666666666666, 'article': 1.0, 'conj': 0.6666666666666666, 'prep': 0.6666666666666666, 'auxverb': 0.6666666666666666, 'adverb': 1.0, 'negate': 0.3333333333333333, 'quant': 0.0}\nbaseline_a_b:  {'ppron': 1.0, 'ipron': 1.0, 'article': 1.0, 'conj': 1.0, 'prep': 0.5, 'auxverb': 1.0, 'adverb': 1.0, 'negate': 1.0, 'quant': 0.0}\n\nC_b_a:  {'ppron': 0.0, 'ipron': 0.0, 'article': 0.0, 'conj': 0.0, 'prep': 0.0, 'auxverb': 0.0, 'adverb': 0.0, 'negate': 0.0, 'quant': None}\nC_a_b:  {'ppron': 0.5, 'ipron': 0.0, 'article': 0.0, 'conj': 0.5, 'prep': 0.0, 'auxverb': 0.0, 'adverb': 0.0, 'negate': 0.5, 'quant': None}\n\nLSM:  {'ppron': 0.6666777774074197, 'ipron': 1.0, 'article': 0.8000039999200016, 'conj': 1.0, 'prep': 0.6666777774074197, 'auxverb': 1.0, 'adverb': 0.8000039999200016, 'negate': 0.6666777774074197, 'quant': None}\n\nmean_C_b_a:  0.0\nmean_C_a_b:  0.1875\n"
     ]
    }
   ],
   "source": [
    "def initialize_dict():\n",
    "    return {\n",
    "        'ppron' : 0,\n",
    "        'ipron' : 0,\n",
    "        'article' : 0,\n",
    "        'conj' : 0,\n",
    "        'prep' : 0,\n",
    "        'auxverb' : 0,\n",
    "        'adverb' : 0,\n",
    "        'negate' : 0,\n",
    "        'quant' : 0\n",
    "    }\n",
    "\n",
    "def measure_coordination(conv, print_output=False):\n",
    "    '''\n",
    "    Assumes the converation will only have two speakers\n",
    "\n",
    "    ~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~\n",
    "        > conv - entire conversation object\n",
    "        > print_output - whether to print medial variables\n",
    "    ~~~~~~~~~~~~ RETURNS ~~~~~~~~~~~~\n",
    "        > C - dictionary with following key value pairs\n",
    "            convID : ID of conversation\n",
    "            a : ID of speaker a\n",
    "            b : ID of speaker b\n",
    "            num_response_b_a : number of responses from b to a\n",
    "            num_response_a_b : number of responses from a to b\n",
    "            C_b_a : dictrionary of asymmetric accomodation from speaker b to speaker a\n",
    "            C_a_b : dictionary of asymmetric accomodation from speaker a to speaker b\n",
    "            LSM : dictionary of symmetric accomodation between both speakers\n",
    "            mean_C_b_a : average accomodation from b towards a across valid markers\n",
    "            mean_C_a_b : average accomodation from a towards b across valid markers\n",
    "            mean_LSM : average of symmetric accommodation\n",
    "            valid_markers : list of valid markers\n",
    "    '''\n",
    "    # ~~~~~~~~~~~ VARIABLES ~~~~~~~~~~~\n",
    "    #     > pairs - dictionary containing interlocution information\n",
    "    #     > raw_b_a - number of style markers used in all responses from b to a\n",
    "    #     > raw_a_b - number of style markers used in all responses from a to b\n",
    "    #     > baseline_b_a - probability of style markers in b's response to a\n",
    "    #     > baseline_a_b - probability of style markers in a's response to b\n",
    "    #     > elicit_b_a - probability of style markers in b's response to a given a exhibited the same marker\n",
    "    #     > elicit_a_b - probability of style markers in a's response to a given b exhibited the same marker\n",
    "    \n",
    "    speakers, utts = get_speaker_utt_lists(conv)\n",
    "    pairs = get_pairs(speakers, utts)\n",
    "\n",
    "    # Get markers from speaker a to b\n",
    "    # Note the order of a_b switched to b_a here. This is to be consistent with\n",
    "    # the notation of C(b,a) indicating the coordination of b to a\n",
    "    elicit_b_a = initialize_dict()\n",
    "    baseline_b_a = initialize_dict()\n",
    "    for a_b in pairs['a_b']:\n",
    "        u_a  = corpus.get_utterance(a_b[0])\n",
    "        u_b = corpus.get_utterance(a_b[1])\n",
    "        m_u_a = get_style_markers(u_a)\n",
    "        m_u_b = get_style_markers(u_b)\n",
    "        for k in m_u_a:\n",
    "            if m_u_a[k]:\n",
    "                if m_u_a[k] == m_u_b[k]:  # If b responded to a with same style marker\n",
    "                    elicit_b_a[k] += 1\n",
    "            baseline_b_a[k] += m_u_b[k] # b's response contains m regardless of a's prompt\n",
    "    \n",
    "    # Get markers from speaker b to a\n",
    "    elicit_a_b = initialize_dict()\n",
    "    baseline_a_b = initialize_dict()\n",
    "    for b_a in pairs['b_a']:\n",
    "        u_b  = corpus.get_utterance(b_a[0])\n",
    "        u_a = corpus.get_utterance(b_a[1])\n",
    "        m_u_a = get_style_markers(u_a)\n",
    "        m_u_b = get_style_markers(u_b)\n",
    "        for k in m_u_b:  \n",
    "            if m_u_b[k]:\n",
    "                if m_u_b[k] == m_u_a[k]:  # If a responded to b with same style marker\n",
    "                    elicit_a_b[k] += 1\n",
    "            baseline_a_b[k] += m_u_a[k] # If a's response contains m regardless of b's prompt\n",
    "    \n",
    "    \n",
    "    # Convert to probabilities, preserving raw baselines for LSM calculation\n",
    "    raw_b_a = baseline_b_a.copy()\n",
    "    raw_a_b = baseline_a_b.copy()\n",
    "    num_response_b_a = len(pairs['a_b'])  # Number of responses from b to a\n",
    "    num_response_a_b = len(pairs['b_a'])  # Number of responses from a to b\n",
    "    for k in elicit_a_b.keys():  # Could be any dictionary, they all have the same keys\n",
    "        elicit_b_a[k] = elicit_b_a[k] / num_response_b_a \n",
    "        baseline_b_a[k] = baseline_b_a[k] / num_response_b_a\n",
    "        elicit_a_b[k] = elicit_a_b[k] / num_response_a_b\n",
    "        baseline_a_b[k] = baseline_a_b[k] / num_response_a_b\n",
    "\n",
    "    # Determine asymmetric and symmetric accomodation\n",
    "    C_b_a = initialize_dict() # Accomodation of b towards a\n",
    "    C_a_b = initialize_dict() # Accomodation of a towards b\n",
    "    LSM = initialize_dict()\n",
    "    for k in C_b_a.keys():\n",
    "        if baseline_b_a[k] and baseline_a_b[k]:  # If a and b both exhibited marker m at some point\n",
    "            C_b_a[k] = baseline_b_a[k] - elicit_b_a[k]\n",
    "            C_a_b[k] = baseline_a_b[k] - elicit_a_b[k]\n",
    "            LSM[k] = 1 - abs(raw_a_b[k] - raw_b_a[k]) / (raw_a_b[k] + raw_b_a[k] + 0.0001)\n",
    "        else:                                    # Else, the metric is undefined for marker m\n",
    "            C_b_a[k] = None  # Set to None if there is no data\n",
    "            C_a_b[k] = None\n",
    "            LSM[k] = None\n",
    "\n",
    "    # Get averages across asymmetric measure\n",
    "    valid_markers = []\n",
    "    mean_C_b_a = 0\n",
    "    mean_C_a_b = 0\n",
    "    mean_LSM = 0\n",
    "    for k in C_b_a.keys():\n",
    "        if C_b_a[k] is not None:\n",
    "            mean_C_b_a += C_b_a[k]\n",
    "            mean_C_a_b += C_a_b[k]\n",
    "            mean_LSM += LSM[k]\n",
    "            valid_markers.append(k)\n",
    "    if valid_markers:\n",
    "        mean_C_b_a /= len(valid_markers)\n",
    "        mean_C_a_b /= len(valid_markers)\n",
    "        mean_LSM /= len(valid_markers)\n",
    "\n",
    "    # Construct dictionary to return\n",
    "    C = {\n",
    "        'convID' : conv.id,\n",
    "        'a' : pairs['a'],\n",
    "        'b' : pairs['b'],\n",
    "        'num_response_b_a' : len(pairs['b_a']),\n",
    "        'num_response_a_b' : len(pairs['a_b']),\n",
    "        'C_b_a' : C_b_a,\n",
    "        'C_a_b' : C_a_b,\n",
    "        'LSM' : LSM,\n",
    "        'mean_C_b_a' : mean_C_b_a,\n",
    "        'mean_C_a_b' : mean_C_a_b,\n",
    "        'mean_LSM' : mean_LSM,\n",
    "        'valid_markers' : valid_markers\n",
    "    } \n",
    "\n",
    "    if print_output:\n",
    "        print('pairs: ', pairs)\n",
    "        print('\\nraw_b_a: ', raw_b_a)\n",
    "        print('raw_a_b: ', raw_a_b)\n",
    "        print('\\nelicit_b_a: ', elicit_b_a)\n",
    "        print('elicit_a_b: ', elicit_a_b)\n",
    "        print('\\nbaseline_b_a: ', baseline_b_a)\n",
    "        print('baseline_a_b: ', baseline_a_b)\n",
    "        print('\\nC_b_a: ', C_b_a)\n",
    "        print('C_a_b: ', C_a_b)\n",
    "        print('\\nLSM: ', LSM)\n",
    "        print('\\nmean_C_b_a: ', mean_C_b_a)\n",
    "        print('mean_C_a_b: ', mean_C_a_b)\n",
    "        \n",
    "    return C\n",
    "\n",
    "\n",
    "conv = corpus.get_conversation(valid_conv_ids[20])\n",
    "C = measure_coordination(conv, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "convID : 162766001.2409.2409\na : The undertow\nb : Indubitably\nnum_response_b_a : 2\nnum_response_a_b : 3\n\n~~ C_b_a ~~\n     ppron : 0.00\n     ipron : 0.00\n     article : 0.00\n     conj : 0.00\n     prep : 0.00\n     auxverb : 0.00\n     adverb : 0.00\n     negate : 0.00\n     quant : None\n\n~~ C_a_b ~~\n     ppron : 0.50\n     ipron : 0.00\n     article : 0.00\n     conj : 0.50\n     prep : 0.00\n     auxverb : 0.00\n     adverb : 0.00\n     negate : 0.50\n     quant : None\n\n~~ LSM ~~\n     ppron : 0.67\n     ipron : 1.00\n     article : 0.80\n     conj : 1.00\n     prep : 0.67\n     auxverb : 1.00\n     adverb : 0.80\n     negate : 0.67\n     quant : None\n\n\nmean_C_b_a : 0.0\nmean_C_a_b : 0.1875\nmean_LSM : 0.8250051665077829\nvalid_markers : ['ppron', 'ipron', 'article', 'conj', 'prep', 'auxverb', 'adverb', 'negate']\n"
     ]
    }
   ],
   "source": [
    "def print_coordination(C):\n",
    "    '''\n",
    "    Prints a coordination dictionary (output from measure_coordination) legibly\n",
    "    '''\n",
    "    for k in C.keys():\n",
    "        if isinstance(C[k], dict):\n",
    "            print('\\n~~ {} ~~'.format(k))\n",
    "            for m in C[k].keys():\n",
    "                if C[k][m] is not None:\n",
    "                    print('     {} : {:.2f}'.format(m, C[k][m]))\n",
    "                else:\n",
    "                    print('     {} : None'.format(m))\n",
    "            if k == 'LSM':\n",
    "                print('\\n')\n",
    "        else:\n",
    "            print('{} : {}'.format(k, C[k]))\n",
    "\n",
    "print_coordination(C)"
   ]
  }
 ]
}